import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import spectral.io.envi as envi
from pathlib import Path
import numpy as np
from einops import rearrange, repeat
from sklearn.metrics import confusion_matrix
import time
import torch.nn.functional as F
import json
import random

device = "cuda" if torch.cuda.is_available() else "cpu"

class Residual(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(x, **kwargs) + x

class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)

class Attention(nn.Module):
    def __init__(self, dim, heads, dim_head, dropout):
        super().__init__()
        inner_dim = dim_head * heads
        self.heads = heads
        self.scale = dim_head ** -0.5
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        )
    def forward(self, x, mask = None):
        b, n, _, h = *x.shape, self.heads
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)
        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale
        mask_value = -torch.finfo(dots.dtype).max
        if mask is not None:
            mask = mask[:, None, :] * mask[:, :, None]
            dots.masked_fill_(~mask, mask_value)
        attn = dots.softmax(dim=-1)
        out = torch.einsum('bhij,bhjd->bhid', attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        out = self.to_out(out)
        return out

class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                Residual(PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))),
                Residual(PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)))
            ]))
    def forward(self, x, mask = None):
        for attn, ff in self.layers:
            x = attn(x, mask = mask)
            x = ff(x)
        return x

class ViT(nn.Module):
    def __init__(self, num_patches, patch_dim, num_classes, dim=64, depth=5, heads=4, 
                 mlp_dim=128, dropout=0.1, emb_dropout=0.1):
        super().__init__()
        
        self.num_patches = num_patches
        self.patch_dim = patch_dim
        
        # Patch embedding
        self.patch_to_embedding = nn.Linear(patch_dim, dim)
        
        # Position embedding
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        
        # CLS token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        
        self.dropout = nn.Dropout(emb_dropout)
        
        # Transformer
        self.transformer = Transformer(dim, depth, heads, 16, mlp_dim, dropout)
        
        # Classification head
        self.to_latent = nn.Identity()
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )
        
    def forward(self, x, mask=None):
        # x: [batch, num_patches, patch_dim]
        
        # Patch embedding
        x = self.patch_to_embedding(x)  # [batch, num_patches, dim]
        
        b, n, _ = x.shape
        
        # Add CLS token
        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)
        x = torch.cat((cls_tokens, x), dim=1)  # [batch, n+1, dim]
        
        # Add position embedding
        x += self.pos_embedding[:, :(n + 1)]
        
        x = self.dropout(x)
        
        # Transformer
        x = self.transformer(x, mask)
        
        # Take CLS token
        x = self.to_latent(x[:, 0])
        
        # Classification
        return self.mlp_head(x)

class ViTDataset(Dataset):
    def __init__(self, root_dir, patch_size=5, stride=5, 
                 class_mapping_file=None, file_list=None):
        self.root_dir = Path(root_dir)
        self.patch_size = patch_size
        self.stride = stride
        self.samples = []
        self.labels = []
        self.original_class_names = []
        self.mapped_class_names = []
        
        # Charger le mapping des classes
        self.class_mapping = {}
        if class_mapping_file is not None:
            with open(class_mapping_file, 'r') as f:
                self.class_mapping = json.load(f)
            print(f"Mapping des classes charg√©: {self.class_mapping}")
        
        # Obtenir la liste des fichiers
        if file_list is not None:
            hdr_files = [f for f in sorted(self.root_dir.rglob("*.hdr")) if f.name in file_list]
        else:
            hdr_files = sorted(self.root_dir.rglob("*.hdr"))
        
        # Premier passage pour identifier toutes les classes originales
        original_class_set = set()
        for hdr_file in hdr_files:
            name = hdr_file.stem
            prefix = name.split("_")[0]
            original_class_set.add(prefix)
        
        self.original_class_names = sorted(list(original_class_set))
        print(f"Classes originales trouv√©es: {self.original_class_names}")
        
        # Cr√©er le mapping des classes originales vers les nouvelles classes
        self.original_to_mapped = {}
        self.mapped_class_names = []
        
        if self.class_mapping:
            for old_class in self.original_class_names:
                found = False
                for new_class, old_classes in self.class_mapping.items():
                    if old_class in old_classes:
                        self.original_to_mapped[old_class] = new_class
                        if new_class not in self.mapped_class_names:
                            self.mapped_class_names.append(new_class)
                        found = True
                        break
                if not found:
                    print(f"Attention: La classe {old_class} n'est pas mapp√©e, elle sera ignor√©e")
        else:
            for old_class in self.original_class_names:
                self.original_to_mapped[old_class] = old_class
            self.mapped_class_names = self.original_class_names.copy()
        
        self.mapped_class_names = sorted(self.mapped_class_names)
        self.class_to_idx = {name: idx for idx, name in enumerate(self.mapped_class_names)}
        self.num_classes = len(self.mapped_class_names)
        
        print(f"Classes apr√®s mapping: {self.mapped_class_names}")
        print(f"Nombre de classes finales: {self.num_classes}")
        
        # Charger et pr√©parer les donn√©es
        all_data = []
        all_labels = []
        
        for hdr_file in hdr_files:
            name = hdr_file.stem
            prefix = name.split("_")[0]
            
            if prefix not in self.original_to_mapped:
                continue
            
            mapped_class_name = self.original_to_mapped[prefix]
            class_id = self.class_to_idx[mapped_class_name]
            
            img = envi.open(str(hdr_file))
            cube = np.array(img.load(), dtype=np.float32)
            
            # Mirror padding
            padded_cube = self._mirror_hsi(cube, patch_size)
            cube_tensor = torch.from_numpy(padded_cube).float()
            
            # Extraire les patches
            patches = self._extract_patches(cube_tensor, patch_size, stride)
            
            for patch in patches:
                all_data.append(patch)
                all_labels.append(class_id)
        
        self.data = np.array(all_data)
        self.labels = np.array(all_labels)
        
        print(f"Donn√©es charg√©es: {self.data.shape}")
        print(f"Labels: {self.labels.shape}")
        
    def _mirror_hsi(self, cube, patch):
        B, H, W = cube.shape
        padding = patch // 2
        mirror_hsi = np.zeros((B, H + 2*padding, W + 2*padding), dtype=float)
        mirror_hsi[:, padding:padding+H, padding:padding+W] = cube
        for i in range(padding):
            mirror_hsi[:, padding:padding+H, i] = cube[:, :, padding-i-1]
        for i in range(padding):
            mirror_hsi[:, padding:padding+H, W+padding+i] = cube[:, :, W-1-i]
        for i in range(padding):
            mirror_hsi[:, i, :] = mirror_hsi[:, padding*2-i-1, :]
        for i in range(padding):
            mirror_hsi[:, H+padding+i, :] = mirror_hsi[:, H+padding-1-i, :]
        return mirror_hsi
    
    def _extract_patches(self, cube_tensor, patch_size, stride):
        B, H, W = cube_tensor.shape
        cube_tensor = cube_tensor.unsqueeze(0)
        patches = F.unfold(cube_tensor, kernel_size=(patch_size, patch_size), dilation=1, stride=stride)
        L = patches.shape[-1]
        patches = patches.squeeze(0).reshape(B, patch_size * patch_size, L).permute(2, 0, 1)
        patches = patches.reshape(L, B, patch_size, patch_size)
        patch_list = []
        for i in range(L):
            patch = patches[i].numpy()
            patch_list.append(patch)
        return patch_list
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]  # Shape: [num_bands, patch_size, patch_size]
        y = self.labels[idx]
        
        # Pour ViT, on aplatit chaque patch en un vecteur
        # [num_bands, patch_size, patch_size] -> [num_bands * patch_size * patch_size]
        x = x.flatten()
        
        return torch.FloatTensor(x), torch.LongTensor([y])[0]

def split_files_by_class(hdr_files, class_mapping, train_ratio=0.7, random_seed=42):
    random.seed(random_seed)
    
    files_by_original_class = {}
    for f in hdr_files:
        prefix = f.stem.split("_")[0]
        if prefix not in files_by_original_class:
            files_by_original_class[prefix] = []
        files_by_original_class[prefix].append(f)
    
    original_to_mapped = {}
    for new_class, old_classes in class_mapping.items():
        for old_class in old_classes:
            original_to_mapped[old_class] = new_class
    
    files_by_mapped_class = {}
    for old_class, files in files_by_original_class.items():
        if old_class in original_to_mapped:
            mapped_class = original_to_mapped[old_class]
            if mapped_class not in files_by_mapped_class:
                files_by_mapped_class[mapped_class] = []
            files_by_mapped_class[mapped_class].extend(files)
    
    train_files = []
    test_files = []
    
    for mapped_class, files in files_by_mapped_class.items():
        random.shuffle(files)
        n_files = len(files)
        
        if n_files == 1:
            print(f"\nüî¥ Classe {mapped_class}: 1 seul fichier - {files[0].name}")
            print(f"   ‚Üí Ce fichier sera utilis√© en TRAIN uniquement")
            train_files.append(files[0])
        elif n_files == 2:
            train_files.append(files[0])
            test_files.append(files[1])
            print(f"üü° Classe {mapped_class}: {n_files} fichiers ‚Üí 1 train, 1 test")
        else:
            n_train = max(1, int(train_ratio * n_files))
            n_test = n_files - n_train
            train_files.extend(files[:n_train])
            test_files.extend(files[n_train:])
            print(f"üü¢ Classe {mapped_class}: {n_files} fichiers ‚Üí {n_train} train, {n_test} test")
    
    return train_files, test_files

class AvgrageMeter(object):
    def __init__(self):
        self.reset()
    def reset(self):
        self.avg = 0
        self.sum = 0
        self.cnt = 0
    def update(self, val, n=1):
        self.sum += val * n
        self.cnt += n
        self.avg = self.sum / self.cnt

def accuracy(output, target, topk=(1,)):
    maxk = max(topk)
    batch_size = target.size(0)
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    res = []
    for k in topk:
        correct_k = correct[:k].view(-1).float().sum(0)
        res.append(correct_k.mul_(100.0/batch_size))
    return res, target, pred.squeeze()

def cal_results(matrix):
    shape = np.shape(matrix)
    number = 0
    sum_val = 0
    AA = np.zeros([shape[0]], dtype=np.float)
    for i in range(shape[0]):
        number += matrix[i, i]
        AA[i] = matrix[i, i] / np.sum(matrix[i, :])
        sum_val += np.sum(matrix[i, :]) * np.sum(matrix[:, i])
    OA = number / np.sum(matrix)
    AA_mean = np.mean(AA)
    pe = sum_val / (np.sum(matrix) ** 2)
    Kappa = (OA - pe) / (1 - pe)
    return OA, AA_mean, Kappa, AA

def output_metric(tar, pre):
    matrix = confusion_matrix(tar, pre)
    OA, AA_mean, Kappa, AA = cal_results(matrix)
    return OA, AA_mean, Kappa, AA

def train_epoch(model, train_loader, criterion, optimizer):
    objs = AvgrageMeter()
    top1 = AvgrageMeter()
    tar = np.array([])
    pre = np.array([])
    model.train()
    for batch_idx, (batch_data, batch_target) in enumerate(train_loader):
        batch_data = batch_data.to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        batch_pred = model(batch_data)
        loss = criterion(batch_pred, batch_target)
        loss.backward()
        optimizer.step()
        prec1, t, p = accuracy(batch_pred, batch_target, topk=(1,))
        n = batch_data.shape[0]
        objs.update(loss.item(), n)
        top1.update(prec1[0].item(), n)
        tar = np.append(tar, t.cpu().numpy())
        pre = np.append(pre, p.cpu().numpy())
    return top1.avg, objs.avg, tar, pre

def test_epoch(model, test_loader, criterion):
    model.eval()
    tar = np.array([])
    pre = np.array([])
    with torch.no_grad():
        for batch_idx, (batch_data, batch_target) in enumerate(test_loader):
            batch_data = batch_data.to(device)
            batch_target = batch_target.to(device)
            batch_pred = model(batch_data)
            _, t, p = accuracy(batch_pred, batch_target, topk=(1,))
            tar = np.append(tar, t.cpu().numpy())
            pre = np.append(pre, p.cpu().numpy())
    return tar, pre

def main():
    # Param√®tres
    data_dir = "/chemin/vers/vos/donnees"
    class_mapping_file = "mapping.json"
    batch_size = 32
    patch_size = 5
    stride = 5
    epochs = 100
    learning_rate = 1e-4
    dim = 64
    depth = 4
    heads = 4
    mlp_dim = 128
    test_freq = 5
    train_ratio = 0.7
    random_seed = 42
    
    print("="*50)
    print("CONFIGURATION ViT")
    print("="*50)
    print(f"Data directory: {data_dir}")
    print(f"Mapping file: {class_mapping_file}")
    print(f"Batch size: {batch_size}")
    print(f"Patch size: {patch_size}")
    print(f"Stride: {stride}")
    print(f"Epochs: {epochs}")
    print(f"Learning rate: {learning_rate}")
    print(f"Dimension: {dim}")
    print(f"Depth: {depth}")
    print(f"Heads: {heads}")
    print(f"Train ratio: {train_ratio}")
    print("="*50)
    
    # Charger le mapping des classes
    with open(class_mapping_file, 'r') as f:
        class_mapping = json.load(f)
    
    # R√©cup√©rer tous les fichiers
    all_hdr_files = sorted(Path(data_dir).rglob("*.hdr"))
    print(f"\nTotal fichiers trouv√©s: {len(all_hdr_files)}")
    
    # Diviser les fichiers
    train_files, test_files = split_files_by_class(
        all_hdr_files,
        class_mapping,
        train_ratio=train_ratio,
        random_seed=random_seed
    )
    
    print(f"\nFichiers train: {len(train_files)}")
    print(f"Fichiers test: {len(test_files)}")
    
    # Cr√©er les datasets
    print("\nChargement du dataset train...")
    train_dataset = ViTDataset(
        root_dir=data_dir,
        patch_size=patch_size,
        stride=stride,
        class_mapping_file=class_mapping_file,
        file_list=[f.name for f in train_files]
    )
    
    print("\nChargement du dataset test...")
    test_dataset = ViTDataset(
        root_dir=data_dir,
        patch_size=patch_size,
        stride=stride,
        class_mapping_file=class_mapping_file,
        file_list=[f.name for f in test_files]
    )
    
    print(f"\nPatches train: {len(train_dataset)}")
    print(f"Patches test: {len(test_dataset)}")
    
    # DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    
    # Calculer les dimensions
    sample_data, _ = train_dataset[0]
    patch_dim = sample_data.shape[0]  # num_bands * patch_size * patch_size
    num_patches = 1  # Pour ViT, chaque patch est un token
    
    print(f"\nDimensions:")
    print(f"Patch dim: {patch_dim}")
    print(f"Num patches: {num_patches}")
    
    # Cr√©er le mod√®le ViT
    model = ViT(
        num_patches=num_patches,
        patch_dim=patch_dim,
        num_classes=train_dataset.num_classes,
        dim=dim,
        depth=depth,
        heads=heads,
        mlp_dim=mlp_dim,
        dropout=0.1,
        emb_dropout=0.1
    ).to(device)
    
    print(f"\nMod√®le cr√©√© avec {sum(p.numel() for p in model.parameters())} param√®tres")
    print(f"Nombre de classes: {train_dataset.num_classes}")
    
    # Crit√®re et optimiseur
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=epochs//10, gamma=0.9)
    
    # Entra√Ænement
    print("\nD√©but de l'entra√Ænement...")
    tic = time.time()
    
    best_acc = 0
    for epoch in range(epochs):
        scheduler.step()
        
        train_acc, train_loss, tar_t, pre_t = train_epoch(
            model, train_loader, criterion, optimizer
        )
        
        print(f"Epoch: {epoch+1:03d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        
        if (epoch % test_freq == 0) or (epoch == epochs - 1):
            tar_test, pre_test = test_epoch(model, test_loader, criterion)
            OA, AA_mean, Kappa, AA = output_metric(tar_test, pre_test)
            print(f"Test - OA: {OA:.4f} | AA: {AA_mean:.4f} | Kappa: {Kappa:.4f}")
            
            if OA > best_acc:
                best_acc = OA
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_acc': best_acc,
                    'class_names': train_dataset.mapped_class_names
                }, 'best_vit_model.pt')
                print(f"  ‚Üí Meilleur mod√®le sauvegard√© (OA: {best_acc:.4f})")
    
    toc = time.time()
    print(f"\nTemps d'entra√Ænement: {toc-tic:.2f} secondes")
    
    # Test final
    print("\n" + "="*50)
    print("√âVALUATION FINALE SUR LE TEST SET")
    print("="*50)
    
    checkpoint = torch.load('best_vit_model.pt')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    tar_test, pre_test = test_epoch(model, test_loader, criterion)
    OA, AA_mean, Kappa, AA = output_metric(tar_test, pre_test)
    
    print(f"Overall Accuracy (OA): {OA:.4f}")
    print(f"Average Accuracy (AA): {AA_mean:.4f}")
    print(f"Kappa Coefficient: {Kappa:.4f}")
    print("\nAccuracy par classe:")
    for i, acc in enumerate(AA):
        class_name = train_dataset.mapped_class_names[i]
        print(f"  Classe {class_name}: {acc:.4f}")
    print("="*50)

if __name__ == "__main__":
    main()