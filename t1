import numpy as np
import matplotlib.pyplot as plt
import sif_parser  
from sif_parser.utils import extract_calibration

# ---------------------------
# Charger les données
# ---------------------------
file_path = "chemin/vers/fichier.sif" 
data, info = sif_parser.np_open(file_path)

# data.shape == (22500, 1, 2048)
# Supprimer la dimension singleton
data = data[:, 0, :]
print("Shape after flattening:", data.shape)  # (22500, 2048)

# ---------------------------
# Reformater le cube
# ---------------------------
height, width = 150, 150
datacube = data.reshape(height, width, -1)
print("Datacube shape:", datacube.shape)  # (150, 150, 2048)

# ---------------------------
# Extraire la calibration
# ---------------------------
wavelengths = extract_calibration(info)
if wavelengths is None:
    raise ValueError("Aucune calibration trouvée dans le fichier SIF")

# ---------------------------
# Trouver l'indice correspondant à 266 nm
# ---------------------------
target_wl = 266.0
idx = np.argmin(np.abs(wavelengths - target_wl))
print(f"Indice correspondant à {target_wl} nm : {idx}, longueur d'onde exacte : {wavelengths[idx]:.2f} nm")

# ---------------------------
# Extraire la cartographie à 266 nm
# ---------------------------
image_266 = datacube[:, :, idx]

# ---------------------------
# Afficher la cartographie
# ---------------------------
plt.figure(figsize=(6, 5))
plt.imshow(image_266, cmap='inferno')
plt.colorbar(label='Intensité')
plt.title(f'Cartographie à {wavelengths[idx]:.2f} nm')
plt.xlabel('Pixel X')
plt.ylabel('Pixel Y')
plt.show()
##########################################################################
class F2D:
    def __init__(self, data):
        self.data = np.array(data, dtype=float)
        self.H, self.W = self.data.shape

    def __call__(self, y, x):
        return self.data[y, x]

    def set(self, y, x, v):
        self.data[y, x] = v

    def copy(self):
        return F2D(self.data.copy())
ImD = F2D(image_266)

def cross_correlation(ImD, iLine, iShift, nMargin):
    W = ImD.W
    n = W - 2 * nMargin
    if n < 1:
        return 0.0

    IMoyCur = 0.0
    IMoyUD = 0.0

    for i in range(nMargin, W - nMargin):
        IMoyCur += ImD(iLine, i + iShift)
        IMoyUD += ImD(iLine - 1, i) + ImD(iLine + 1, i)

    IMoyCur /= n
    IMoyUD /= n

    SumNum = SumCur = SumUD = 0.0

    for i in range(nMargin, W - nMargin):
        xCur = ImD(iLine, i + iShift) - IMoyCur
        xUD = (ImD(iLine - 1, i) + ImD(iLine + 1, i)) - IMoyUD
        SumNum += xCur * xUD
        SumCur += xCur * xCur
        SumUD += xUD * xUD

    return SumNum / np.sqrt(1e-30 + SumCur * SumUD)
def find_line_offsets_iter(ImD, nMargin):
    W, H = ImD.W, ImD.H
    ImDCor = ImD.copy()
    LineOff = np.zeros(H, dtype=int)
    sumOffset = 0

    for j in range(1, H - 1):
        CCk = []

        for k in range(-nMargin, nMargin + 1):
            CCk.append(cross_correlation(ImD, j, k, nMargin))

        CCk = np.array(CCk)
        CCMax = CCk.max()
        kMax = np.where(CCk == CCMax)[0][0] - nMargin

        if CCMax > 0.2:
            LineOff[j] = kMax
            sumOffset += abs(kMax)

            for i in range(W):
                src = (i + kMax) % W
                ImDCor.set(j, i, ImD(j, src))

    return ImDCor, LineOff, sumOffset
def find_line_offsets(ImD, maxIter, nMargin):
    H = ImD.H
    LineOffsets = np.zeros(H, dtype=int)
    ImDCur = ImD.copy()

    for _ in range(maxIter):
        ImDCor, curOff, nUpdate = find_line_offsets_iter(ImDCur, nMargin)
        LineOffsets += curOff
        ImDCur = ImDCor
        if nUpdate == 0:
            break

    return LineOffsets
nMargin = 5
maxIter = 10

LineOffsets = find_line_offsets(ImD, maxIter, nMargin)
def correct_datacube(datacube, LineOffsets):
    H, W, Z = datacube.shape
    corrected = datacube.copy()

    for j in range(H):
        kMax = LineOffsets[j]
        for i in range(W):
            src = (i + kMax) % W
            corrected[j, i, :] = datacube[j, src, :]

    return corrected

datacube_corrected = correct_datacube(datacube, LineOffsets)

image_266_cor = datacube_corrected[:, :, idx]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.imshow(image_266, cmap='inferno')
plt.title("Avant correction")
plt.colorbar()

plt.subplot(1, 2, 2)
plt.imshow(image_266_cor, cmap='inferno')
plt.title("Après correction")
plt.colorbar()

plt.show()


##################3
import numpy as np
import matplotlib.pyplot as plt
import sif_parser
from sif_parser.utils import extract_calibration
from numba import njit, prange

# ---------------------------
# Charger les données
# ---------------------------
file_path = "chemin/vers/fichier.sif"
data, info = sif_parser.np_open(file_path)
data = data[:, 0, :]  # Supprimer dimension singleton

height, width = 150, 150
datacube = data.reshape(height, width, -1)  # Cube H x W x Z

wavelengths = extract_calibration(info)
if wavelengths is None:
    raise ValueError("Aucune calibration trouvée dans le fichier SIF")

target_wl = 266.0
idx = np.argmin(np.abs(wavelengths - target_wl))
image_266 = datacube[:, :, idx]

plt.figure(figsize=(6, 5))
plt.imshow(image_266, cmap='inferno')
plt.colorbar(label='Intensité')
plt.title(f'Cartographie à {wavelengths[idx]:.2f} nm')
plt.show()


@njit
def cross_correlation_numba(ImD, iLine, iShift, nMargin):
    W = ImD.shape[1]
    n = W - 2 * nMargin
    if n < 1:
        return 0.0

    IMoyCur = 0.0
    IMoyUD = 0.0
    for i in range(nMargin, W - nMargin):
        IMoyCur += ImD[iLine, i + iShift]
        IMoyUD += ImD[iLine - 1, i] + ImD[iLine + 1, i]

    IMoyCur /= n
    IMoyUD /= n

    SumNum = 0.0
    SumCur = 0.0
    SumUD = 0.0
    for i in range(nMargin, W - nMargin):
        xCur = ImD[iLine, i + iShift] - IMoyCur
        xUD = (ImD[iLine - 1, i] + ImD[iLine + 1, i]) - IMoyUD
        SumNum += xCur * xUD
        SumCur += xCur * xCur
        SumUD += xUD * xUD

    return SumNum / np.sqrt(1e-30 + SumCur * SumUD)


@njit(parallel=True)
def find_line_offsets_iter_numba(ImD, nMargin):
    H, W = ImD.shape
    LineOff = np.zeros(H, dtype=np.int32)
    ImDCor = ImD.copy()

    for j in prange(1, H - 1):
        CCk = np.empty(2 * nMargin + 1, dtype=np.float64)
        for k in range(-nMargin, nMargin + 1):
            CCk[k + nMargin] = cross_correlation_numba(ImD, j, k, nMargin)

        CCMax = CCk.max()
        kMax = np.argmax(CCk) - nMargin

        if CCMax > 0.2:
            LineOff[j] = kMax
            for i in range(W):
                src = (i + kMax) % W
                ImDCor[j, i] = ImD[j, src]

    return ImDCor, LineOff

def find_line_offsets(ImD, maxIter, nMargin):
    LineOffsets = np.zeros(ImD.shape[0], dtype=np.int32)
    ImDCur = ImD.copy()
    for _ in range(maxIter):
        ImDCor, curOff = find_line_offsets_iter_numba(ImDCur, nMargin)
        LineOffsets += curOff
        if np.all(curOff == 0):
            break
        ImDCur = ImDCor
    return LineOffsets

@njit(parallel=True)
def correct_datacube_numba(datacube, LineOffsets):
    H, W, Z = datacube.shape
    corrected = np.empty_like(datacube)
    for j in prange(H):
        kMax = LineOffsets[j]
        for i in range(W):
            src = (i + kMax) % W
            corrected[j, i, :] = datacube[j, src, :]
    return corrected

nMargin = 3
maxIter = 10

LineOffsets = find_line_offsets(image_266, maxIter, nMargin)
datacube_corrected = correct_datacube_numba(datacube, LineOffsets)

image_266_cor = datacube_corrected[:, :, idx]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.imshow(image_266, cmap='inferno')
plt.title("Avant correction")
plt.colorbar()

plt.subplot(1, 2, 2)
plt.imshow(image_266_cor, cmap='inferno')
plt.title("Après correction")
plt.colorbar()
plt.show()


def calc_average_spectrum(datacube):
    """
    datacube : (H, W, Z)
    """
    mean_spectrum = datacube.mean(axis=(0, 1))
    max_intensity = mean_spectrum.max()
    return mean_spectrum, 

import tkinter as tk
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

def show_mean_spectrum_tk(datacube, wavelengths):
    mean_spectrum, max_intensity = calc_average_spectrum(datacube)

    root = tk.Tk()
    root.title("Spectre moyen LIBS")

    fig, ax = plt.subplots(figsize=(7, 4))
    ax.plot(wavelengths, mean_spectrum, color="blue")
    ax.set_xlabel("Longueur d'onde (nm)")
    ax.set_ylabel("Intensité moyenne")
    ax.set_title("Spectre moyen hyperspectral")
    ax.grid(True)

    # affichage de l'intensité max
    ax.text(
        0.02, 0.95,
        f"Max = {max_intensity:.2f}",
        transform=ax.transAxes,
        verticalalignment='top'
    )

    canvas = FigureCanvasTkAgg(fig, master=root)
    canvas.draw()
    canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

    root.mainloop()

show_mean_spectrum_tk(datacube, wavelengths)
mean_before, _ = calc_average_spectrum(datacube)
mean_after, _ = calc_average_spectrum(datacube_corrected)

plt.figure()
plt.plot(wavelengths, mean_before, label="Avant")
plt.plot(wavelengths, mean_after, label="Après")
plt.legend()
plt.title("Comparaison spectre moyen")
plt.show()


def calc_average_spectrum(datacube):
    mean_spectrum = datacube.mean(axis=(0, 1))
    return mean_spectrum

def crop_datacube_spectral(datacube, wavelengths, wl_min, wl_max):
    mask = (wavelengths >= wl_min) & (wavelengths <= wl_max)
    cropped_cube = datacube[:, :, mask]
    cropped_wl = wavelengths[mask]
    return cropped_cube, cropped_wl

import tkinter as tk
from tkinter import messagebox
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
import numpy as np
import matplotlib.pyplot as plt


datacube_cropped = None
wavelengths_cropped = None

def show_mean_spectrum_tk(datacube, wavelengths):
    global datacube_cropped, wavelengths_cropped

    mean_spectrum = calc_average_spectrum(datacube)

    root = tk.Tk()
    root.title("Spectre moyen LIBS – Zoom & Crop")

    fig, ax = plt.subplots(figsize=(8, 4))
    ax.plot(wavelengths, mean_spectrum, color="blue")
    ax.set_xlabel("Longueur d'onde (nm)")
    ax.set_ylabel("Intensité moyenne")
    ax.grid(True)

    canvas = FigureCanvasTkAgg(fig, master=root)
    canvas.draw()
    canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

    frame = tk.Frame(root)
    frame.pack(pady=5)

    tk.Label(frame, text="λ min").grid(row=0, column=0)
    entry_min = tk.Entry(frame, width=8)
    entry_min.grid(row=0, column=1)
    entry_min.insert(0, f"{wavelengths.min():.1f}")

    tk.Label(frame, text="λ max").grid(row=0, column=2)
    entry_max = tk.Entry(frame, width=8)
    entry_max.grid(row=0, column=3)
    entry_max.insert(0, f"{wavelengths.max():.1f}")

    def apply_crop():
        global datacube_cropped, wavelengths_cropped

        wl_min = float(entry_min.get())
        wl_max = float(entry_max.get())

        datacube_cropped, wavelengths_cropped = crop_datacube_spectral(
            datacube, wavelengths, wl_min, wl_max
        )

        ax.clear()
        ax.plot(wavelengths_cropped,
                datacube_cropped.mean(axis=(0, 1)),
                color="red")
        ax.set_xlim(wl_min, wl_max)
        ax.set_title("Spectre moyen coupé")
        ax.grid(True)
        canvas.draw()

    tk.Button(frame, text="Appliquer crop", command=apply_crop)\
        .grid(row=1, column=0, columnspan=4, pady=5)

    root.mainloop()

datacube_cropped.shape
# (150, 150, Z_crop)

wavelengths_cropped.shape
# (Z_crop,)


show_mean_spectrum_tk(datacube_corrected, wavelengths)


import math
import numpy as np

class IFFAlgorithm:
    def __init__(self, a_init, m_inst):
        self.a_Init = a_init
        self.m_Inst = m_inst

    # Equivalent de void SetRandomGen(int choice)
    def SetRandomGen(self, choice):
        self.a_Init.SetRandomGen(choice)

    # Equivalent de GetFreq(array<int>^% index, array<int>^% freq)
    def GetFreq(self):
        vec_index = []
        vec_freq = []
        self.a_Init.GetFreq(vec_index, vec_freq)

        if len(vec_index) != len(vec_freq) or len(vec_index) == 0 or len(vec_freq) == 0:
            return [], []

        # Copier les valeurs directement dans des listes Python
        index = [i for i in vec_index]
        freq = [f for f in vec_freq]
        return index, freq

    # Equivalent de GetFreqGrouped(array<array<int>^>^% index, array<int>^% freq)
    def GetFreqGrouped(self):
        vec_index = []
        vec_freq = []
        self.m_Inst.GetFreqGrouped(vec_index, vec_freq)

        if len(vec_index) == 0 or len(vec_freq) == 0 or len(vec_index) != len(vec_freq):
            return [], []

        index = []
        freq = [0] * len(vec_freq)
        for i, class_vec in enumerate(vec_index):
            if len(class_vec) > 0:
                index.append([v for v in class_vec])
            else:
                index.append([])  # Resize empty
            freq[i] = 0
        return index, freq

    # Equivalent de Compute(ProgressBar^% pb, RichTextBox^% rtb)
    def Compute(self, pb=None, rtb=None):
        npix = self.m_Inst.NPix()
        ns = self.m_Inst.NSample()

        if rtb: rtb.Refresh()
        if pb:
            pb.Minimum = 0
            pb.Maximum = npix - 1
            pb.Value = 0

        # Center data
        for i in range(npix):
            self.m_Inst.Center_data(i)
            if pb:
                pb.Value = i
                pb.Refresh()

        if rtb: 
            rtb.Text = "Sampling data by random vectors"
            rtb.Refresh()

        if pb:
            pb.Minimum = 0
            pb.Maximum = ns - 1
            pb.Value = 0

        if self.m_Inst.RandomGen() == 1:
            for i in range(ns):
                self.a_Init.MCstep_wu(i)
                if pb:
                    pb.Value = i
                    pb.Refresh()
        else:
            for i in range(ns):
                self.m_Inst.MCstep_bs(i)
                if pb:
                    pb.Value = i
                    pb.Refresh()

        self.m_Inst.Sortvotes()


# Classe IFF avec MCstep et centrage
class IFF:
    def MCstep_bs(self):
        imin = 0
        imax = 0
        minproj = float('inf')
        maxproj = -float('inf')

        self.rand_vec_bs()  # _rnd_vec must be updated here

        for i in range(self._npix):
            spec_centered = self.getspectrumcentered(i)
            proj = sum(self._rnd_vec[k] * spec_centered[k] for k in range(self._ncha))

            if proj < minproj:
                imin = i
                minproj = proj
            if proj > maxproj:
                imax = i
                maxproj = proj

        self._votes[imin] += 1
        self._votes[imax] += 1

    def MCstep_wu(self):
        imin = 0
        imax = 0
        minproj = float('inf')
        maxproj = -float('inf')

        self.rand_vec_mu()  # _rnd_vec must be updated here

        for i in range(self._npix):
            spec_centered = self.getspectrumcentered(i)
            proj = sum(self._rnd_vec[k] * spec_centered[k] for k in range(self._ncha))

            if proj < minproj:
                imin = i
                minproj = proj
            if proj > maxproj:
                imax = i
                maxproj = proj

        self._votes[imin] += 1
        self._votes[imax] += 1

    def Center_data(self, id):
        spec = self.getspectrum(id)
        spec_centered = self.getspectrumcentered(id)
        mean = sum(spec) / self._ncha
        for k in range(self._ncha):
            spec_centered[k] = spec[k] - mean

    def Sortvotes(self):
        self._freq.clear()
        for i in range(self._npix):
            if self._votes[i] > 0:
                self._freq.append((i, self._votes[i]))
        self._freq.sort(key=lambda x: x[1], reverse=True)

    def correl_coef(self, sp1, sp2):
        sumxy = sum(sp1[k] * sp2[k] for k in range(self._ncha))
        sumx2 = sum(sp1[k] ** 2 for k in range(self._ncha))
        sumy2 = sum(sp2[k] ** 2 for k in range(self._ncha))
        return sumxy / math.sqrt(sumx2 * sumy2)

    def rand_vect_mu(self):
        self._rnd_vec = [2.0 * np.random.rand() - 1.0 for _ in range(self._ncha)]

    def rand_vec_bs(self):
        vec = np.random.normal(0, 1, self._ncha)
        norm = np.linalg.norm(vec)
        self._rnd_vec = (vec / norm).tolist()

    # Accès aux spectres (comme pointers C++)
    def getspectrum(self, index):
        return self._data[index]

    def getspectrumcentered(self, index):
        return self._data_centered[index]



# =========================
# Algorithme IFF
# =========================
class IFFAlgorithm:
    def __init__(self, data, n_samples, random_gen=0):
        """
        data: numpy array of shape (n_pixels, n_channels)
        n_samples: number of Monte Carlo iterations
        random_gen: 0 = Schnabel & Janke, 1 = Wu et al.
        """
        self.data = data
        self.npix, self.ncha = data.shape
        self.n_samples = n_samples
        self.random_gen = random_gen
        self.votes = np.zeros(self.npix, dtype=int)
        self.data_centered = np.zeros_like(self.data)
        self.freq = []
        self.freq_grouped = []

    def set_random_gen(self, choice):
        self.random_gen = choice

    def center_data(self):
        """Center all spectra"""
        self.data_centered = self.data - self.data.mean(axis=1, keepdims=True)

    def compute(self):
        """Compute the IFF algorithm"""
        self.center_data()
        for i in range(self.n_samples):
            if self.random_gen == 0:
                self.mc_step_bs()
            else:
                self.mc_step_wu()
        self.sort_votes()

    def mc_step_bs(self):
        """Monte Carlo step Schnabel & Janke"""
        rnd_vec = self.rand_vect_bs()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def mc_step_wu(self):
        """Monte Carlo step Wu et al."""
        rnd_vec = self.rand_vect_mu()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def sort_votes(self):
        """Sort by descending frequency"""
        idx = np.nonzero(self.votes)[0]
        freq = self.votes[idx]
        sorted_idx = np.argsort(-freq)
        self.freq = [(int(idx[i]), int(freq[i])) for i in sorted_idx]

    def get_freq(self):
        """Return indices and frequencies"""
        if not self.freq:
            return np.array([]), np.array([])
        indices, freqs = zip(*self.freq)
        return np.array(indices, dtype=int), np.array(freqs, dtype=int)

    def group_vertices(self, correlconf=0.9):
        """Group spectra with correlation > correlconf"""
        grouped = []
        remaining = self.freq.copy()
        while remaining:
            ref_idx, ref_freq = remaining.pop(0)
            ref_spec = self.data[ref_idx]
            group = [ref_idx]
            total_freq = ref_freq
            to_remove = []
            for i, (idx, freq) in enumerate(remaining):
                corr = self.correl_coef(ref_spec, self.data[idx])
                if corr > correlconf:
                    group.append(idx)
                    total_freq += freq
                    to_remove.append(i)
            for i in reversed(to_remove):
                remaining.pop(i)
            grouped.append((group, total_freq))
        grouped.sort(key=lambda x: -x[1])
        self.freq_grouped = grouped

    @staticmethod
    def correl_coef(sp1, sp2):
        """Correlation coefficient between two spectra"""
        return np.dot(sp1, sp2) / (np.linalg.norm(sp1) * np.linalg.norm(sp2))

    def rand_vect_mu(self):
        """Random vector uniform in [-1,1]"""
        return np.random.uniform(-1, 1, self.ncha)

    def rand_vect_bs(self):
        """Random vector on unit sphere (approx.)"""
        vec = np.random.normal(0, 1, self.ncha)
        return vec / np.linalg.norm(vec)


# =========================
# Préparer les données pour IFF
# =========================
H, W, Z = datacube_corrected.shape
data_2D = datacube_corrected.reshape(H*W, Z)  # mise en 2D (pixels x canaux)
print("Shape pour IFF:", data_2D.shape)

# =========================
# Appliquer IFF
# =========================
iff = IFFAlgorithm(data_2D, n_samples=500, random_gen=0)
iff.compute()

indices, freqs = iff.get_freq()
print("Indices top 10 trouvés:", indices[:10])
print("Votes correspondants:", freqs[:10])

iff.group_vertices(correlconf=0.9)
print("Nombre de groupes formés:", len(iff.freq_grouped))

# =========================
# Visualiser les pixels les plus votés sur l'image
# =========================
top_pixels = indices[:20]  # par exemple top 20
mask = np.zeros(H*W, dtype=bool)
mask[top_pixels] = True
vote_map = mask.reshape(H, W)

plt.figure(figsize=(6,5))
plt.imshow(vote_map, cmap='hot')
plt.title("Pixels les plus votés par IFF")
plt.colorbar()
plt.show()


import numpy as np
import matplotlib.pyplot as plt
import sif_parser
from sif_parser.utils import extract_calibration
from sklearn.decomposition import PCA

# =========================
# Chargement des données
# =========================
file_path = "chemin/vers/fichier.sif"
data, info = sif_parser.np_open(file_path)
data = data[:, 0, :]  # Supprimer dimension singleton
print("Shape after flattening:", data.shape)  # (22500, 2048)

# ---------------------------
# Reformater le cube
# ---------------------------
height, width = 150, 150
datacube = data.reshape(height, width, -1)
print("Datacube shape:", datacube.shape)  # (150, 150, 2048)

# ---------------------------
# Extraire la calibration
# ---------------------------
wavelengths = extract_calibration(info)
if wavelengths is None:
    raise ValueError("Aucune calibration trouvée dans le fichier SIF")

# ---------------------------
# Trouver l'indice correspondant à 266 nm
# ---------------------------
target_wl = 266.0
idx = np.argmin(np.abs(wavelengths - target_wl))
print(f"Indice correspondant à {target_wl} nm : {idx}, longueur d'onde exacte : {wavelengths[idx]:.2f} nm")

# ---------------------------
# Extraire la cartographie à 266 nm
# ---------------------------
image_266 = datacube[:, :, idx]

plt.figure(figsize=(6, 5))
plt.imshow(image_266, cmap='inferno')
plt.colorbar(label='Intensité')
plt.title(f'Cartographie à {wavelengths[idx]:.2f} nm')
plt.show()

# =========================
# Algorithme IFF
# =========================
class IFFAlgorithm:
    def __init__(self, data, n_samples, random_gen=0):
        self.data = data
        self.npix, self.ncha = data.shape
        self.n_samples = n_samples
        self.random_gen = random_gen
        self.votes = np.zeros(self.npix, dtype=int)
        self.data_centered = np.zeros_like(self.data)
        self.freq = []
        self.freq_grouped = []

    def set_random_gen(self, choice):
        self.random_gen = choice

    def center_data(self):
        self.data_centered = self.data - self.data.mean(axis=1, keepdims=True)

    def compute(self):
        self.center_data()
        for i in range(self.n_samples):
            if self.random_gen == 0:
                self.mc_step_bs()
            else:
                self.mc_step_wu()
        self.sort_votes()

    def mc_step_bs(self):
        rnd_vec = self.rand_vect_bs()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def mc_step_wu(self):
        rnd_vec = self.rand_vect_mu()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def sort_votes(self):
        idx = np.nonzero(self.votes)[0]
        freq = self.votes[idx]
        sorted_idx = np.argsort(-freq)
        self.freq = [(int(idx[i]), int(freq[i])) for i in sorted_idx]

    def get_freq(self):
        if not self.freq:
            return np.array([]), np.array([])
        indices, freqs = zip(*self.freq)
        return np.array(indices, dtype=int), np.array(freqs, dtype=int)

    def group_vertices(self, correlconf=0.9):
        grouped = []
        remaining = self.freq.copy()
        while remaining:
            ref_idx, ref_freq = remaining.pop(0)
            ref_spec = self.data[ref_idx]
            group = [ref_idx]
            total_freq = ref_freq
            to_remove = []
            for i, (idx, freq) in enumerate(remaining):
                corr = self.correl_coef(ref_spec, self.data[idx])
                if corr > correlconf:
                    group.append(idx)
                    total_freq += freq
                    to_remove.append(i)
            for i in reversed(to_remove):
                remaining.pop(i)
            grouped.append((group, total_freq))
        grouped.sort(key=lambda x: -x[1])
        self.freq_grouped = grouped

    @staticmethod
    def correl_coef(sp1, sp2):
        return np.dot(sp1, sp2) / (np.linalg.norm(sp1) * np.linalg.norm(sp2))

    def rand_vect_mu(self):
        return np.random.uniform(-1, 1, self.ncha)

    def rand_vect_bs(self):
        vec = np.random.normal(0, 1, self.ncha)
        return vec / np.linalg.norm(vec)

# =========================
# Préparer les données pour IFF
# =========================
H, W, Z = datacube.shape
data_2D = datacube.reshape(H*W, Z)
print("Shape pour IFF:", data_2D.shape)

# =========================
# Appliquer IFF
# =========================
iff = IFFAlgorithm(data_2D, n_samples=500, random_gen=0)
iff.compute()
indices, freqs = iff.get_freq()
print("Indices top 10 trouvés:", indices[:10])
print("Votes correspondants:", freqs[:10])

iff.group_vertices(correlconf=0.9)
print("Nombre de groupes formés:", len(iff.freq_grouped))

# =========================
# Visualiser pixels les plus votés
# =========================
top_pixels = indices[:20]
mask = np.zeros(H*W, dtype=bool)
mask[top_pixels] = True
vote_map = mask.reshape(H, W)

plt.figure(figsize=(6,5))
plt.imshow(vote_map, cmap='hot')
plt.title("Pixels les plus votés par IFF")
plt.colorbar()
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

def visualize_iff_results(datacube, iff, group=False):
    H, W, Z = datacube.shape
    pixels = datacube.reshape(-1, Z)  # (npix, n_channels)

    # PCA pour réduire à 4 composantes principales
    pca = PCA(n_components=4)
    pixels_pca = pca.fit_transform(pixels)  # shape (npix, 4)

    # Définir les couples de composantes à afficher
    pc_pairs = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]
    n_pairs = len(pc_pairs)

    plt.figure(figsize=(15, 10))

    for i, (pcx, pcy) in enumerate(pc_pairs):
        plt.subplot(2, 3, i+1)
        # Afficher tous les pixels en gris
        plt.scatter(pixels_pca[:, pcx], pixels_pca[:, pcy], c='lightgray', alpha=0.5, label='Non sélectionnés')

        if group:
            # Afficher les groupes
            colors = plt.cm.get_cmap('tab10', len(iff.freq_grouped))
            for j, (group_indices, total_votes) in enumerate(iff.freq_grouped):
                coords = pixels_pca[group_indices]
                plt.scatter(coords[:, pcx], coords[:, pcy], color=colors(j), s=20, label=f'Groupe {j+1}')
        else:
            # Afficher les pixels sélectionnés
            selected_indices, _ = iff.get_freq()
            plt.scatter(pixels_pca[selected_indices, pcx], pixels_pca[selected_indices, pcy],
                        c='red', s=20, label='Pixels sélectionnés')

        plt.xlabel(f'PC {pcx+1}')
        plt.ylabel(f'PC {pcy+1}')
        plt.title(f'PC{pcx+1} vs PC{pcy+1}')
        plt.grid(True)

    plt.tight_layout()
    plt.show()


# Visualiser pixels sélectionnés
visualize_iff_results(datacube, iff, group=False)

# Visualiser groupes
visualize_iff_results(datacube, iff, group=True)
