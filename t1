import numpy as np
import matplotlib.pyplot as plt
import sif_parser  
from sif_parser.utils import extract_calibration

# ---------------------------
# Charger les donn√©es
# ---------------------------
file_path = "chemin/vers/fichier.sif" 
data, info = sif_parser.np_open(file_path)

# data.shape == (22500, 1, 2048)
# Supprimer la dimension singleton
data = data[:, 0, :]
print("Shape after flattening:", data.shape)  # (22500, 2048)

# ---------------------------
# Reformater le cube
# ---------------------------
height, width = 150, 150
datacube = data.reshape(height, width, -1)
print("Datacube shape:", datacube.shape)  # (150, 150, 2048)

# ---------------------------
# Extraire la calibration
# ---------------------------
wavelengths = extract_calibration(info)
if wavelengths is None:
    raise ValueError("Aucune calibration trouv√©e dans le fichier SIF")

# ---------------------------
# Trouver l'indice correspondant √† 266 nm
# ---------------------------
target_wl = 266.0
idx = np.argmin(np.abs(wavelengths - target_wl))
print(f"Indice correspondant √† {target_wl} nm : {idx}, longueur d'onde exacte : {wavelengths[idx]:.2f} nm")

# ---------------------------
# Extraire la cartographie √† 266 nm
# ---------------------------
image_266 = datacube[:, :, idx]

# ---------------------------
# Afficher la cartographie
# ---------------------------
plt.figure(figsize=(6, 5))
plt.imshow(image_266, cmap='inferno')
plt.colorbar(label='Intensit√©')
plt.title(f'Cartographie √† {wavelengths[idx]:.2f} nm')
plt.xlabel('Pixel X')
plt.ylabel('Pixel Y')
plt.show()
##########################################################################
class F2D:
    def __init__(self, data):
        self.data = np.array(data, dtype=float)
        self.H, self.W = self.data.shape

    def __call__(self, y, x):
        return self.data[y, x]

    def set(self, y, x, v):
        self.data[y, x] = v

    def copy(self):
        return F2D(self.data.copy())
ImD = F2D(image_266)

def cross_correlation(ImD, iLine, iShift, nMargin):
    W = ImD.W
    n = W - 2 * nMargin
    if n < 1:
        return 0.0

    IMoyCur = 0.0
    IMoyUD = 0.0

    for i in range(nMargin, W - nMargin):
        IMoyCur += ImD(iLine, i + iShift)
        IMoyUD += ImD(iLine - 1, i) + ImD(iLine + 1, i)

    IMoyCur /= n
    IMoyUD /= n

    SumNum = SumCur = SumUD = 0.0

    for i in range(nMargin, W - nMargin):
        xCur = ImD(iLine, i + iShift) - IMoyCur
        xUD = (ImD(iLine - 1, i) + ImD(iLine + 1, i)) - IMoyUD
        SumNum += xCur * xUD
        SumCur += xCur * xCur
        SumUD += xUD * xUD

    return SumNum / np.sqrt(1e-30 + SumCur * SumUD)
def find_line_offsets_iter(ImD, nMargin):
    W, H = ImD.W, ImD.H
    ImDCor = ImD.copy()
    LineOff = np.zeros(H, dtype=int)
    sumOffset = 0

    for j in range(1, H - 1):
        CCk = []

        for k in range(-nMargin, nMargin + 1):
            CCk.append(cross_correlation(ImD, j, k, nMargin))

        CCk = np.array(CCk)
        CCMax = CCk.max()
        kMax = np.where(CCk == CCMax)[0][0] - nMargin

        if CCMax > 0.2:
            LineOff[j] = kMax
            sumOffset += abs(kMax)

            for i in range(W):
                src = (i + kMax) % W
                ImDCor.set(j, i, ImD(j, src))

    return ImDCor, LineOff, sumOffset
def find_line_offsets(ImD, maxIter, nMargin):
    H = ImD.H
    LineOffsets = np.zeros(H, dtype=int)
    ImDCur = ImD.copy()

    for _ in range(maxIter):
        ImDCor, curOff, nUpdate = find_line_offsets_iter(ImDCur, nMargin)
        LineOffsets += curOff
        ImDCur = ImDCor
        if nUpdate == 0:
            break

    return LineOffsets
nMargin = 5
maxIter = 10

LineOffsets = find_line_offsets(ImD, maxIter, nMargin)
def correct_datacube(datacube, LineOffsets):
    H, W, Z = datacube.shape
    corrected = datacube.copy()

    for j in range(H):
        kMax = LineOffsets[j]
        for i in range(W):
            src = (i + kMax) % W
            corrected[j, i, :] = datacube[j, src, :]

    return corrected

datacube_corrected = correct_datacube(datacube, LineOffsets)

image_266_cor = datacube_corrected[:, :, idx]

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.imshow(image_266, cmap='inferno')
plt.title("Avant correction")
plt.colorbar()

plt.subplot(1, 2, 2)
plt.imshow(image_266_cor, cmap='inferno')
plt.title("Apr√®s correction")
plt.colorbar()

plt.show()


##################3
import numpy as np
import matplotlib.pyplot as plt
import sif_parser
from sif_parser.utils import extract_calibration
from numba import njit, prange

# ---------------------------
# Charger les donn√©es
# ---------------------------
file_path = "chemin/vers/fichier.sif"
data, info = sif_parser.np_open(file_path)
data = data[:, 0, :]  # Supprimer dimension singleton

height, width = 150, 150
datacube = data.reshape(height, width, -1)  # Cube H x W x Z

wavelengths = extract_calibration(info)
if wavelengths is None:
    raise ValueError("Aucune calibration trouv√©e dans le fichier SIF")

target_wl = 266.0
idx = np.argmin(np.abs(wavelengths - target_wl))
image_266 = datacube[:, :, idx]

plt.figure(figsize=(6, 5))
plt.imshow(image_266, cmap='inferno')
plt.colorbar(label='Intensit√©')
plt.title(f'Cartographie √† {wavelengths[idx]:.2f} nm')
plt.show()


@njit
def cross_correlation_numba(ImD, iLine, iShift, nMargin):
    W = ImD.shape[1]
    n = W - 2 * nMargin
    if n < 1:
        return 0.0

    IMoyCur = 0.0
    IMoyUD = 0.0
    for i in range(nMargin, W - nMargin):
        IMoyCur += ImD[iLine, i + iShift]
        IMoyUD += ImD[iLine - 1, i] + ImD[iLine + 1, i]

    IMoyCur /= n
    IMoyUD /= n

    SumNum = 0.0
    SumCur = 0.0
    SumUD = 0.0
    for i in range(nMargin, W - nMargin):
        xCur = ImD[iLine, i + iShift] - IMoyCur
        xUD = (ImD[iLine - 1, i] + ImD[iLine + 1, i]) - IMoyUD
        SumNum += xCur * xUD
        SumCur += xCur * xCur
        SumUD += xUD * xUD

    return SumNum / np.sqrt(1e-30 + SumCur * SumUD)


@njit(parallel=True)
def find_line_offsets_iter_numba(ImD, nMargin):
    H, W = ImD.shape
    LineOff = np.zeros(H, dtype=np.int32)
    ImDCor = ImD.copy()

    for j in prange(1, H - 1):
        CCk = np.empty(2 * nMargin + 1, dtype=np.float64)
        for k in range(-nMargin, nMargin + 1):
            CCk[k + nMargin] = cross_correlation_numba(ImD, j, k, nMargin)

        CCMax = CCk.max()
        kMax = np.argmax(CCk) - nMargin

        if CCMax > 0.2:
            LineOff[j] = kMax
            for i in range(W):
                src = (i + kMax) % W
                ImDCor[j, i] = ImD[j, src]

    return ImDCor, LineOff

def find_line_offsets(ImD, maxIter, nMargin):
    LineOffsets = np.zeros(ImD.shape[0], dtype=np.int32)
    ImDCur = ImD.copy()
    for _ in range(maxIter):
        ImDCor, curOff = find_line_offsets_iter_numba(ImDCur, nMargin)
        LineOffsets += curOff
        if np.all(curOff == 0):
            break
        ImDCur = ImDCor
    return LineOffsets

@njit(parallel=True)
def correct_datacube_numba(datacube, LineOffsets):
    H, W, Z = datacube.shape
    corrected = np.empty_like(datacube)
    for j in prange(H):
        kMax = LineOffsets[j]
        for i in range(W):
            src = (i + kMax) % W
            corrected[j, i, :] = datacube[j, src, :]
    return corrected

nMargin = 3
maxIter = 10

LineOffsets = find_line_offsets(image_266, maxIter, nMargin)
datacube_corrected = correct_datacube_numba(datacube, LineOffsets)

image_266_cor = datacube_corrected[:, :, idx]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.imshow(image_266, cmap='inferno')
plt.title("Avant correction")
plt.colorbar()

plt.subplot(1, 2, 2)
plt.imshow(image_266_cor, cmap='inferno')
plt.title("Apr√®s correction")
plt.colorbar()
plt.show()


def calc_average_spectrum(datacube):
    """
    datacube : (H, W, Z)
    """
    mean_spectrum = datacube.mean(axis=(0, 1))
    max_intensity = mean_spectrum.max()
    return mean_spectrum, 

import tkinter as tk
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

def show_mean_spectrum_tk(datacube, wavelengths):
    mean_spectrum, max_intensity = calc_average_spectrum(datacube)

    root = tk.Tk()
    root.title("Spectre moyen LIBS")

    fig, ax = plt.subplots(figsize=(7, 4))
    ax.plot(wavelengths, mean_spectrum, color="blue")
    ax.set_xlabel("Longueur d'onde (nm)")
    ax.set_ylabel("Intensit√© moyenne")
    ax.set_title("Spectre moyen hyperspectral")
    ax.grid(True)

    # affichage de l'intensit√© max
    ax.text(
        0.02, 0.95,
        f"Max = {max_intensity:.2f}",
        transform=ax.transAxes,
        verticalalignment='top'
    )

    canvas = FigureCanvasTkAgg(fig, master=root)
    canvas.draw()
    canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

    root.mainloop()

show_mean_spectrum_tk(datacube, wavelengths)
mean_before, _ = calc_average_spectrum(datacube)
mean_after, _ = calc_average_spectrum(datacube_corrected)

plt.figure()
plt.plot(wavelengths, mean_before, label="Avant")
plt.plot(wavelengths, mean_after, label="Apr√®s")
plt.legend()
plt.title("Comparaison spectre moyen")
plt.show()


def calc_average_spectrum(datacube):
    mean_spectrum = datacube.mean(axis=(0, 1))
    return mean_spectrum

def crop_datacube_spectral(datacube, wavelengths, wl_min, wl_max):
    mask = (wavelengths >= wl_min) & (wavelengths <= wl_max)
    cropped_cube = datacube[:, :, mask]
    cropped_wl = wavelengths[mask]
    return cropped_cube, cropped_wl

import tkinter as tk
from tkinter import messagebox
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk
import numpy as np
import matplotlib.pyplot as plt


datacube_cropped = None
wavelengths_cropped = None

def show_mean_spectrum_tk(datacube, wavelengths):
    global datacube_cropped, wavelengths_cropped

    mean_spectrum = calc_average_spectrum(datacube)

    root = tk.Tk()
    root.title("Spectre moyen LIBS ‚Äì Zoom & Crop")

    fig, ax = plt.subplots(figsize=(8, 4))
    ax.plot(wavelengths, mean_spectrum, color="blue")
    ax.set_xlabel("Longueur d'onde (nm)")
    ax.set_ylabel("Intensit√© moyenne")
    ax.grid(True)

    canvas = FigureCanvasTkAgg(fig, master=root)
    canvas.draw()
    canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

    frame = tk.Frame(root)
    frame.pack(pady=5)

    tk.Label(frame, text="Œª min").grid(row=0, column=0)
    entry_min = tk.Entry(frame, width=8)
    entry_min.grid(row=0, column=1)
    entry_min.insert(0, f"{wavelengths.min():.1f}")

    tk.Label(frame, text="Œª max").grid(row=0, column=2)
    entry_max = tk.Entry(frame, width=8)
    entry_max.grid(row=0, column=3)
    entry_max.insert(0, f"{wavelengths.max():.1f}")

    def apply_crop():
        global datacube_cropped, wavelengths_cropped

        wl_min = float(entry_min.get())
        wl_max = float(entry_max.get())

        datacube_cropped, wavelengths_cropped = crop_datacube_spectral(
            datacube, wavelengths, wl_min, wl_max
        )

        ax.clear()
        ax.plot(wavelengths_cropped,
                datacube_cropped.mean(axis=(0, 1)),
                color="red")
        ax.set_xlim(wl_min, wl_max)
        ax.set_title("Spectre moyen coup√©")
        ax.grid(True)
        canvas.draw()

    tk.Button(frame, text="Appliquer crop", command=apply_crop)\
        .grid(row=1, column=0, columnspan=4, pady=5)

    root.mainloop()

datacube_cropped.shape
# (150, 150, Z_crop)

wavelengths_cropped.shape
# (Z_crop,)


show_mean_spectrum_tk(datacube_corrected, wavelengths)


import math
import numpy as np

class IFFAlgorithm:
    def __init__(self, a_init, m_inst):
        self.a_Init = a_init
        self.m_Inst = m_inst

    # Equivalent de void SetRandomGen(int choice)
    def SetRandomGen(self, choice):
        self.a_Init.SetRandomGen(choice)

    # Equivalent de GetFreq(array<int>^% index, array<int>^% freq)
    def GetFreq(self):
        vec_index = []
        vec_freq = []
        self.a_Init.GetFreq(vec_index, vec_freq)

        if len(vec_index) != len(vec_freq) or len(vec_index) == 0 or len(vec_freq) == 0:
            return [], []

        # Copier les valeurs directement dans des listes Python
        index = [i for i in vec_index]
        freq = [f for f in vec_freq]
        return index, freq

    # Equivalent de GetFreqGrouped(array<array<int>^>^% index, array<int>^% freq)
    def GetFreqGrouped(self):
        vec_index = []
        vec_freq = []
        self.m_Inst.GetFreqGrouped(vec_index, vec_freq)

        if len(vec_index) == 0 or len(vec_freq) == 0 or len(vec_index) != len(vec_freq):
            return [], []

        index = []
        freq = [0] * len(vec_freq)
        for i, class_vec in enumerate(vec_index):
            if len(class_vec) > 0:
                index.append([v for v in class_vec])
            else:
                index.append([])  # Resize empty
            freq[i] = 0
        return index, freq

    # Equivalent de Compute(ProgressBar^% pb, RichTextBox^% rtb)
    def Compute(self, pb=None, rtb=None):
        npix = self.m_Inst.NPix()
        ns = self.m_Inst.NSample()

        if rtb: rtb.Refresh()
        if pb:
            pb.Minimum = 0
            pb.Maximum = npix - 1
            pb.Value = 0

        # Center data
        for i in range(npix):
            self.m_Inst.Center_data(i)
            if pb:
                pb.Value = i
                pb.Refresh()

        if rtb: 
            rtb.Text = "Sampling data by random vectors"
            rtb.Refresh()

        if pb:
            pb.Minimum = 0
            pb.Maximum = ns - 1
            pb.Value = 0

        if self.m_Inst.RandomGen() == 1:
            for i in range(ns):
                self.a_Init.MCstep_wu(i)
                if pb:
                    pb.Value = i
                    pb.Refresh()
        else:
            for i in range(ns):
                self.m_Inst.MCstep_bs(i)
                if pb:
                    pb.Value = i
                    pb.Refresh()

        self.m_Inst.Sortvotes()


# Classe IFF avec MCstep et centrage
class IFF:
    def MCstep_bs(self):
        imin = 0
        imax = 0
        minproj = float('inf')
        maxproj = -float('inf')

        self.rand_vec_bs()  # _rnd_vec must be updated here

        for i in range(self._npix):
            spec_centered = self.getspectrumcentered(i)
            proj = sum(self._rnd_vec[k] * spec_centered[k] for k in range(self._ncha))

            if proj < minproj:
                imin = i
                minproj = proj
            if proj > maxproj:
                imax = i
                maxproj = proj

        self._votes[imin] += 1
        self._votes[imax] += 1

    def MCstep_wu(self):
        imin = 0
        imax = 0
        minproj = float('inf')
        maxproj = -float('inf')

        self.rand_vec_mu()  # _rnd_vec must be updated here

        for i in range(self._npix):
            spec_centered = self.getspectrumcentered(i)
            proj = sum(self._rnd_vec[k] * spec_centered[k] for k in range(self._ncha))

            if proj < minproj:
                imin = i
                minproj = proj
            if proj > maxproj:
                imax = i
                maxproj = proj

        self._votes[imin] += 1
        self._votes[imax] += 1

    def Center_data(self, id):
        spec = self.getspectrum(id)
        spec_centered = self.getspectrumcentered(id)
        mean = sum(spec) / self._ncha
        for k in range(self._ncha):
            spec_centered[k] = spec[k] - mean

    def Sortvotes(self):
        self._freq.clear()
        for i in range(self._npix):
            if self._votes[i] > 0:
                self._freq.append((i, self._votes[i]))
        self._freq.sort(key=lambda x: x[1], reverse=True)

    def correl_coef(self, sp1, sp2):
        sumxy = sum(sp1[k] * sp2[k] for k in range(self._ncha))
        sumx2 = sum(sp1[k] ** 2 for k in range(self._ncha))
        sumy2 = sum(sp2[k] ** 2 for k in range(self._ncha))
        return sumxy / math.sqrt(sumx2 * sumy2)

    def rand_vect_mu(self):
        self._rnd_vec = [2.0 * np.random.rand() - 1.0 for _ in range(self._ncha)]

    def rand_vec_bs(self):
        vec = np.random.normal(0, 1, self._ncha)
        norm = np.linalg.norm(vec)
        self._rnd_vec = (vec / norm).tolist()

    # Acc√®s aux spectres (comme pointers C++)
    def getspectrum(self, index):
        return self._data[index]

    def getspectrumcentered(self, index):
        return self._data_centered[index]



# =========================
# Algorithme IFF
# =========================
class IFFAlgorithm:
    def __init__(self, data, n_samples, random_gen=0):
        """
        data: numpy array of shape (n_pixels, n_channels)
        n_samples: number of Monte Carlo iterations
        random_gen: 0 = Schnabel & Janke, 1 = Wu et al.
        """
        self.data = data
        self.npix, self.ncha = data.shape
        self.n_samples = n_samples
        self.random_gen = random_gen
        self.votes = np.zeros(self.npix, dtype=int)
        self.data_centered = np.zeros_like(self.data)
        self.freq = []
        self.freq_grouped = []

    def set_random_gen(self, choice):
        self.random_gen = choice

    def center_data(self):
        """Center all spectra"""
        self.data_centered = self.data - self.data.mean(axis=1, keepdims=True)

    def compute(self):
        """Compute the IFF algorithm"""
        self.center_data()
        for i in range(self.n_samples):
            if self.random_gen == 0:
                self.mc_step_bs()
            else:
                self.mc_step_wu()
        self.sort_votes()

    def mc_step_bs(self):
        """Monte Carlo step Schnabel & Janke"""
        rnd_vec = self.rand_vect_bs()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def mc_step_wu(self):
        """Monte Carlo step Wu et al."""
        rnd_vec = self.rand_vect_mu()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def sort_votes(self):
        """Sort by descending frequency"""
        idx = np.nonzero(self.votes)[0]
        freq = self.votes[idx]
        sorted_idx = np.argsort(-freq)
        self.freq = [(int(idx[i]), int(freq[i])) for i in sorted_idx]

    def get_freq(self):
        """Return indices and frequencies"""
        if not self.freq:
            return np.array([]), np.array([])
        indices, freqs = zip(*self.freq)
        return np.array(indices, dtype=int), np.array(freqs, dtype=int)

    def group_vertices(self, correlconf=0.9):
        """Group spectra with correlation > correlconf"""
        grouped = []
        remaining = self.freq.copy()
        while remaining:
            ref_idx, ref_freq = remaining.pop(0)
            ref_spec = self.data[ref_idx]
            group = [ref_idx]
            total_freq = ref_freq
            to_remove = []
            for i, (idx, freq) in enumerate(remaining):
                corr = self.correl_coef(ref_spec, self.data[idx])
                if corr > correlconf:
                    group.append(idx)
                    total_freq += freq
                    to_remove.append(i)
            for i in reversed(to_remove):
                remaining.pop(i)
            grouped.append((group, total_freq))
        grouped.sort(key=lambda x: -x[1])
        self.freq_grouped = grouped

    @staticmethod
    def correl_coef(sp1, sp2):
        """Correlation coefficient between two spectra"""
        return np.dot(sp1, sp2) / (np.linalg.norm(sp1) * np.linalg.norm(sp2))

    def rand_vect_mu(self):
        """Random vector uniform in [-1,1]"""
        return np.random.uniform(-1, 1, self.ncha)

    def rand_vect_bs(self):
        """Random vector on unit sphere (approx.)"""
        vec = np.random.normal(0, 1, self.ncha)
        return vec / np.linalg.norm(vec)


# =========================
# Pr√©parer les donn√©es pour IFF
# =========================
H, W, Z = datacube_corrected.shape
data_2D = datacube_corrected.reshape(H*W, Z)  # mise en 2D (pixels x canaux)
print("Shape pour IFF:", data_2D.shape)

# =========================
# Appliquer IFF
# =========================
iff = IFFAlgorithm(data_2D, n_samples=500, random_gen=0)
iff.compute()

indices, freqs = iff.get_freq()
print("Indices top 10 trouv√©s:", indices[:10])
print("Votes correspondants:", freqs[:10])

iff.group_vertices(correlconf=0.9)
print("Nombre de groupes form√©s:", len(iff.freq_grouped))

# =========================
# Visualiser les pixels les plus vot√©s sur l'image
# =========================
top_pixels = indices[:20]  # par exemple top 20
mask = np.zeros(H*W, dtype=bool)
mask[top_pixels] = True
vote_map = mask.reshape(H, W)

plt.figure(figsize=(6,5))
plt.imshow(vote_map, cmap='hot')
plt.title("Pixels les plus vot√©s par IFF")
plt.colorbar()
plt.show()


import numpy as np
import matplotlib.pyplot as plt
import sif_parser
from sif_parser.utils import extract_calibration
from sklearn.decomposition import PCA

# =========================
# Chargement des donn√©es
# =========================
file_path = "chemin/vers/fichier.sif"
data, info = sif_parser.np_open(file_path)
data = data[:, 0, :]  # Supprimer dimension singleton
print("Shape after flattening:", data.shape)  # (22500, 2048)

# ---------------------------
# Reformater le cube
# ---------------------------
height, width = 150, 150
datacube = data.reshape(height, width, -1)
print("Datacube shape:", datacube.shape)  # (150, 150, 2048)

# ---------------------------
# Extraire la calibration
# ---------------------------
wavelengths = extract_calibration(info)
if wavelengths is None:
    raise ValueError("Aucune calibration trouv√©e dans le fichier SIF")

# ---------------------------
# Trouver l'indice correspondant √† 266 nm
# ---------------------------
target_wl = 266.0
idx = np.argmin(np.abs(wavelengths - target_wl))
print(f"Indice correspondant √† {target_wl} nm : {idx}, longueur d'onde exacte : {wavelengths[idx]:.2f} nm")

# ---------------------------
# Extraire la cartographie √† 266 nm
# ---------------------------
image_266 = datacube[:, :, idx]

plt.figure(figsize=(6, 5))
plt.imshow(image_266, cmap='inferno')
plt.colorbar(label='Intensit√©')
plt.title(f'Cartographie √† {wavelengths[idx]:.2f} nm')
plt.show()

# =========================
# Algorithme IFF
# =========================

BUCKET_SIZE = 16   
import numpy as np
from numba import njit


@njit
def rand_vec_bs_odd_numba(n):
    assert n % 2 == 1, "n doit √™tre impair"
    nb = 1 + n // BUCKET_SIZE
    max_points = n // 2

    # Tableaux pour stocker les points par bucket
    # shape = (nb, max_points, 3) pour (a,b,s)
    buckets = np.zeros((nb, max_points, 3))
    counts = np.zeros(nb, dtype=np.int64)

    # √âtape 1 : g√©n√©ration disque unit√©
    filled = 0
    while filled < max_points:
        a = 2.0 * np.random.rand() - 1.0
        b = 2.0 * np.random.rand() - 1.0
        s = a*a + b*b
        if s <= 1.0:
            bucket = int(s * nb)
            if bucket >= nb:
                bucket = nb - 1
            idx = counts[bucket]
            buckets[bucket, idx, 0] = a
            buckets[bucket, idx, 1] = b
            buckets[bucket, idx, 2] = s
            counts[bucket] += 1
            filled += 1

    # √âtape 2 : tri par s dans chaque bucket
    for b in range(nb):
        c = counts[b]
        if c > 1:
            s_vals = buckets[b, :c, 2]
            idx_sort = np.argsort(s_vals)
            buckets[b, :c, :] = buckets[b, idx_sort, :]

    # √âtape 3 : trouver first et last bucket non vides
    first_bucket = 0
    while first_bucket < nb and counts[first_bucket] == 0:
        first_bucket += 1

    last_bucket = nb - 1
    while last_bucket > 0 and counts[last_bucket] == 0:
        last_bucket -= 1

    # √âtape 4 : initialisation
    a0, b0, s0 = buckets[first_bucket, 0, 0], buckets[first_bucket, 0, 1], buckets[first_bucket, 0, 2]
    s = 1.0 / buckets[last_bucket, counts[last_bucket]-1, 2]
    s = s / (1.0 - s * a0 * a0)

    v = np.zeros(n, dtype=np.float64)
    v[0] = a0 * np.sqrt(s)

    S_II = s0
    idx = 1

    # √âtape 5 : remplissage cumulatif
    for b in range(first_bucket, last_bucket+1):
        for j in range(1, counts[b]):
            a, b_, s_ = buckets[b, j, 0], buckets[b, j, 1], buckets[b, j, 2]
            S_I = S_II
            S_II += s_
            t = np.sqrt((1.0 - S_I / S_II) * s)
            v[idx] = a * t
            v[idx+1] = b_ * t
            idx += 2

    return v


@njit
def rand_vec_bs_even_numba(n):
    assert n % 2 == 0, "n doit √™tre pair"
    nb = 1 + n // BUCKET_SIZE
    max_points = n // 2

    buckets = np.zeros((nb, max_points, 3))
    counts = np.zeros(nb, dtype=np.int64)

    # √âtape 1 : g√©n√©ration disque unit√©
    filled = 0
    while filled < max_points:
        a = 2.0 * np.random.rand() - 1.0
        b = 2.0 * np.random.rand() - 1.0
        s = a*a + b*b
        if s <= 1.0:
            bucket = int(s * nb)
            if bucket >= nb:
                bucket = nb - 1
            idx = counts[bucket]
            buckets[bucket, idx, 0] = a
            buckets[bucket, idx, 1] = b
            buckets[bucket, idx, 2] = s
            counts[bucket] += 1
            filled += 1

    # √âtape 2 : tri par s dans chaque bucket
    for b in range(nb):
        c = counts[b]
        if c > 1:
            s_vals = buckets[b, :c, 2]
            idx_sort = np.argsort(s_vals)
            buckets[b, :c, :] = buckets[b, idx_sort, :]

    # √âtape 3 : trouver last bucket non vide
    last_bucket = nb - 1
    while last_bucket > 0 and counts[last_bucket] == 0:
        last_bucket -= 1

    s = 1.0 / buckets[last_bucket, counts[last_bucket]-1, 2]

    # √âtape 4 : remplissage cumulatif
    v = np.zeros(n, dtype=np.float64)
    idx = 0
    S_II = 0.0

    for b in range(last_bucket+1):
        for j in range(counts[b]):
            a, b_, s_ = buckets[b, j, 0], buckets[b, j, 1], buckets[b, j, 2]
            S_I = S_II
            S_II += s_
            t = np.sqrt((1.0 - S_I / S_II) * s)
            v[idx] = a * t
            v[idx+1] = b_ * t
            idx += 2

    return v

import numpy as np
from numba import njit, prange

# -------------------------------
# Fonctions numba pour le calcul
# -------------------------------

@njit
def F_alpha_numba(spec, kernel, niter):
    n = spec.size
    w = kernel.size
    mid = w // 2
    out = np.empty_like(spec)
    temp = spec.copy()

    for _ in range(niter):
        for i in range(n):
            sumval = 0.0
            for j in range(w):
                k = i - j + mid
                if k < 0:
                    k = 0
                elif k >= n:
                    k = n - 1
                sumval += temp[k] * kernel[j]
            out[i] = temp[i] if temp[i] < sumval else sumval
        temp, out = out, temp  # swap
    return temp

@njit
def remove_baseline_numba(spec, kernel, niter):
    bkg = F_alpha_numba(spec, kernel, niter)
    return spec - bkg

@njit(parallel=True)
def batch_correlation_numba(ref_spec, specs):
    n_specs = specs.shape[0]
    n_chan = specs.shape[1]
    corr = np.empty(n_specs, dtype=np.float32)

    ref_mean = ref_spec.mean()
    ref_std = ref_spec.std()
    ref_centered = ref_spec - ref_mean

    for i in prange(n_specs):
        s = specs[i]
        s_mean = s.mean()
        s_std = s.std()
        if s_std == 0.0 or ref_std == 0.0:
            corr[i] = 0.0
        else:
            s_centered = s - s_mean
            corr[i] = np.sum(ref_centered * s_centered) / (ref_std * s_std * n_chan)
    return corr

# -------------------------------
# Classe BkgCalculator
# -------------------------------
class BkgCalculator:
    def __init__(self, ncha, width=5, niter=5, kernel_type=1):
        self.ncha = ncha
        self.width = width
        self.niter = niter
        self.kernel_type = kernel_type
        if kernel_type > 0:
            # Gaussien
            self._widthkernel = 6 * width + 1
            self._midkernel = 3 * width + 1
            x = np.arange(self._widthkernel) - self._midkernel
            kernel = np.exp(-0.5 * (x / width) ** 2)
        else:
            # Constante
            self._widthkernel = 2 * width + 1
            self._midkernel = width + 1
            kernel = np.ones(self._widthkernel, dtype=np.float32)
        self.kernel = kernel.astype(np.float32) / kernel.sum()

    def removebaseline(self, spec):
        spec = np.array(spec, dtype=np.float32)
        return remove_baseline_numba(spec, self.kernel, self.niter)

class IFFAlgorithm:
    def __init__(self, data, n_samples, random_gen=0):
        self.data = data
        self.npix, self.ncha = data.shape
        self.n_samples = n_samples
        self.random_gen = random_gen
        self.votes = np.zeros(self.npix, dtype=int)
        self.data_centered = np.zeros_like(self.data)
        self.freq = []
        self.freq_grouped = []

    def set_random_gen(self, choice):
        self.random_gen = choice

    def center_data(self):
        self.data_centered = self.data - self.data.mean(axis=1, keepdims=True)

    def compute(self):
        self.center_data()
        for i in range(self.n_samples):
            if self.random_gen == 0:
                self.mc_step_bs()
            else:
                self.mc_step_wu()
        self.sort_votes()

    def mc_step_bs(self):
        rnd_vec = self.rand_vect_bs()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def mc_step_wu(self):
        rnd_vec = self.rand_vect_mu()
        proj = self.data_centered @ rnd_vec
        imin = np.argmin(proj)
        imax = np.argmax(proj)
        self.votes[imin] += 1
        self.votes[imax] += 1

    def sort_votes(self):
        idx = np.nonzero(self.votes)[0]
        freq = self.votes[idx]
        sorted_idx = np.argsort(-freq)
        self.freq = [(int(idx[i]), int(freq[i])) for i in sorted_idx]

    def get_freq(self):
        if not self.freq:
            return np.array([]), np.array([])
        indices, freqs = zip(*self.freq)
        return np.array(indices, dtype=int), np.array(freqs, dtype=int)

    def getspectrum(self, idx):
        return self.data[idx]

    def group_vertices(self, correlconf=0.9, kernel=0, width=5, niter=5):
        self.freq_grouped = []
        toprocess = self.freq.copy()

        if not toprocess:
            return

        if kernel > 0:
            bkgc = BkgCalculator(self.ncha, width, niter, kernel)

        while toprocess:
            ref_idx, ref_freq = toprocess.pop(0)
            ref_spec = self.getspectrum(ref_idx)
            ref_spec = bkgc.removebaseline(ref_spec) if kernel > 0 else ref_spec

            if not toprocess:
                self.freq_grouped.append(([ref_idx], ref_freq))
                continue

            indices, freqs = zip(*toprocess)
            specs_stack = np.array([self.getspectrum(i) for i in indices], dtype=np.float32)
            if kernel > 0:
                specs_stack = np.array([bkgc.removebaseline(s) for s in specs_stack], dtype=np.float32)

            corrs = batch_correlation_numba(ref_spec, specs_stack)
            mask = corrs > correlconf

            group_indices = [ref_idx] + [indices[i] for i, m in enumerate(mask) if m]
            total_freq = ref_freq + sum(freqs[i] for i, m in enumerate(mask) if m)

            toprocess = [(i, f) for i, f in toprocess if i not in group_indices]
            self.freq_grouped.append((group_indices, total_freq))

        self.freq_grouped.sort(key=lambda x: -x[1])

    @staticmethod
    def correl_coef(sp1, sp2):
        return np.dot(sp1, sp2) / (np.linalg.norm(sp1) * np.linalg.norm(sp2))

    def rand_vect_mu(self):
        return np.random.uniform(-1, 1, self.ncha)

    def rand_vect_bs(n):
        if n % 2 == 1:
            return rand_vec_bs_odd_numba(n)
        else:
            return rand_vec_bs_even_numba(n)


# =========================
# Pr√©parer les donn√©es pour IFF
# =========================
H, W, Z = datacube.shape
data_2D = datacube.reshape(H*W, Z)
print("Shape pour IFF:", data_2D.shape)

# =========================
# Appliquer IFF
# =========================
iff = IFFAlgorithm(data_2D, n_samples=500, random_gen=0)
iff.compute()
indices, freqs = iff.get_freq()
print("Indices top 10 trouv√©s:", indices[:10])
print("Votes correspondants:", freqs[:10])

iff.group_vertices(correlconf=0.9)
print("Nombre de groupes form√©s:", len(iff.freq_grouped))

# =========================
# Visualiser pixels les plus vot√©s
# =========================
top_pixels = indices[:20]
mask = np.zeros(H*W, dtype=bool)
mask[top_pixels] = True
vote_map = mask.reshape(H, W)

plt.figure(figsize=(6,5))
plt.imshow(vote_map, cmap='hot')
plt.title("Pixels les plus vot√©s par IFF")
plt.colorbar()
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

def visualize_iff_results(datacube, iff, group=False):
    H, W, Z = datacube.shape
    pixels = datacube.reshape(-1, Z)  # (npix, n_channels)

    # PCA pour r√©duire √† 4 composantes principales
    pca = PCA(n_components=4)
    pixels_pca = pca.fit_transform(pixels)  # shape (npix, 4)

    # D√©finir les couples de composantes √† afficher
    pc_pairs = [(0,1), (0,2), (0,3), (1,2), (1,3), (2,3)]
    n_pairs = len(pc_pairs)

    plt.figure(figsize=(15, 10))

    for i, (pcx, pcy) in enumerate(pc_pairs):
        plt.subplot(2, 3, i+1)
        # Afficher tous les pixels en gris
        plt.scatter(pixels_pca[:, pcx], pixels_pca[:, pcy], c='lightgray', alpha=0.5, label='Non s√©lectionn√©s')

        if group:
            # Afficher les groupes
            colors = plt.cm.get_cmap('tab10', len(iff.freq_grouped))
            for j, (group_indices, total_votes) in enumerate(iff.freq_grouped):
                coords = pixels_pca[group_indices]
                plt.scatter(coords[:, pcx], coords[:, pcy], color=colors(j), s=20, label=f'Groupe {j+1}')
        else:
            # Afficher les pixels s√©lectionn√©s
            selected_indices, _ = iff.get_freq()
            plt.scatter(pixels_pca[selected_indices, pcx], pixels_pca[selected_indices, pcy],
                        c='red', s=20, label='Pixels s√©lectionn√©s')

        plt.xlabel(f'PC {pcx+1}')
        plt.ylabel(f'PC {pcy+1}')
        plt.title(f'PC{pcx+1} vs PC{pcy+1}')
        plt.grid(True)

    plt.tight_layout()
    plt.show()


# Visualiser pixels s√©lectionn√©s
visualize_iff_results(datacube, iff, group=False)

# Visualiser groupes
visualize_iff_results(datacube, iff, group=True)



import numpy as np

def get_freq(self):
    if not self.freq:
        return np.array([]), np.array([])

    n = len(self.freq)
    indices = np.empty(n, dtype=int)
    freqs = np.empty(n, dtype=int)

    for i, f in enumerate(self.freq):
        indices[i] = f[0]
        freqs[i] = f[1]

    return indices, freqs


class IFFAlgorithmStreaming:
    def __init__(self, data, n_samples, random_gen=0, block_size=4096):
        self.data = data                # memmap ou ndarray
        self.npix, self.ncha = data.shape
        self.n_samples = n_samples
        self.random_gen = random_gen
        self.block_size = block_size
        self.votes = np.zeros(self.npix, dtype=np.int32)
        self.freq = []

    # -------------------------
    # G√©n√©rateurs al√©atoires
    # -------------------------
    def rand_vect_bs(self):
        if self.ncha % 2 == 1:
            return rand_vec_bs_odd_numba(self.ncha)
        else:
            return rand_vec_bs_even_numba(self.ncha)

    def rand_vect_mu(self):
        v = np.random.normal(0, 1, self.ncha)
        return v / np.linalg.norm(v)

    # -------------------------
    # IFF streaming
    # -------------------------
    def compute(self):
        for _ in range(self.n_samples):
            if self.random_gen == 0:
                rnd_vec = self.rand_vect_bs()
            else:
                rnd_vec = self.rand_vect_mu()

            gmin = 1e30
            gmax = -1e30
            gmin_idx = 0
            gmax_idx = 0

            # üî• STREAMING ICI
            for i in range(0, self.npix, self.block_size):
                block = self.data[i:i + self.block_size]

                # centrage local (sans stocker data_centered)
                block = block - block.mean(axis=1, keepdims=True)

                proj = block @ rnd_vec   # petit tableau

                imin = np.argmin(proj)
                imax = np.argmax(proj)

                if proj[imin] < gmin:
                    gmin = proj[imin]
                    gmin_idx = i + imin

                if proj[imax] > gmax:
                    gmax = proj[imax]
                    gmax_idx = i + imax

            self.votes[gmin_idx] += 1
            self.votes[gmax_idx] += 1

        self.sort_votes()

    # -------------------------
    # Post-traitement
    # -------------------------
    def sort_votes(self):
        idx = np.nonzero(self.votes)[0]
        freq = self.votes[idx]
        order = np.argsort(-freq)
        self.freq = [(int(idx[i]), int(freq[i])) for i in order]

    def get_freq(self):
        if not self.freq:
            return np.array([]), np.array([])
        indices, freqs = zip(*self.freq)
        return np.array(indices), np.array(freqs)

    def getspectrum(self, idx):
        return self.data[idx]

def group_vertices(self, correlconf=0.9):
    self.freq_grouped = []
    toprocess = self.freq.copy()

    while toprocess:
        ref_idx, ref_freq = toprocess.pop(0)
        ref_spec = self.getspectrum(ref_idx)
        ref_spec = ref_spec - ref_spec.mean()

        group = [ref_idx]
        total_freq = ref_freq

        remaining = []

        for idx, freq in toprocess:
            spec = self.getspectrum(idx)
            spec = spec - spec.mean()

            corr = np.dot(ref_spec, spec) / (
                np.linalg.norm(ref_spec) * np.linalg.norm(spec) + 1e-12
            )

            if corr > correlconf:
                group.append(idx)
                total_freq += freq
            else:
                remaining.append((idx, freq))

        self.freq_grouped.append((group, total_freq))
        toprocess = remaining

    self.freq_grouped.sort(key=lambda x: -x[1])


from numba import njit, prange
import numpy as np

@njit(parallel=True)
def block_projection(block, rnd_vec):
    n_pixels, n_chan = block.shape
    proj = np.empty(n_pixels, dtype=np.float32)
    for i in prange(n_pixels):
        s = 0.0
        for j in range(n_chan):
            s += block[i, j] * rnd_vec[j]
        proj[i] = s
    return proj


class IFFAlgorithmStreamingParallel:
    def __init__(self, data, n_samples, random_gen=0, block_size=4096):
        self.data = data
        self.npix, self.ncha = data.shape
        self.n_samples = n_samples
        self.random_gen = random_gen
        self.block_size = block_size
        self.votes = np.zeros(self.npix, dtype=np.int32)
        self.freq = []

    # Vecteurs al√©atoires
    def rand_vect_bs(self):
        if self.ncha % 2 == 1:
            return rand_vec_bs_odd_numba(self.ncha).astype(np.float32)
        else:
            return rand_vec_bs_even_numba(self.ncha).astype(np.float32)

    def rand_vect_mu(self):
        v = np.random.normal(0, 1, self.ncha).astype(np.float32)
        return v / np.linalg.norm(v)

    # -------------------
    # Compute IFF streaming + parall√©lis√©
    # -------------------
    def compute(self):
        for _ in range(self.n_samples):
            if self.random_gen == 0:
                rnd_vec = self.rand_vect_bs()
            else:
                rnd_vec = self.rand_vect_mu()

            gmin = 1e30
            gmax = -1e30
            gmin_idx = 0
            gmax_idx = 0

            # Stream par blocs
            for i in range(0, self.npix, self.block_size):
                block = self.data[i:i+self.block_size].astype(np.float32)
                block -= block.mean(axis=1, keepdims=True)  # centrage par pixel

                # Projection parall√®le
                proj = block_projection(block, rnd_vec)

                # Min / max global
                imin = np.argmin(proj)
                imax = np.argmax(proj)

                if proj[imin] < gmin:
                    gmin = proj[imin]
                    gmin_idx = i + imin
                if proj[imax] > gmax:
                    gmax = proj[imax]
                    gmax_idx = i + imax

            self.votes[gmin_idx] += 1
            self.votes[gmax_idx] += 1

        self.sort_votes()

    # -------------------
    # Tri des votes
    # -------------------
    def sort_votes(self):
        idx = np.nonzero(self.votes)[0]
        freq = self.votes[idx]
        order = np.argsort(-freq)
        self.freq = [(int(idx[i]), int(freq[i])) for i in order]

    # R√©cup√©rer fr√©quences et spectres
    def get_freq(self):
        if not self.freq:
            return np.array([]), np.array([])
        indices, freqs = zip(*self.freq)
        return np.array(indices), np.array(freqs)

    def getspectrum(self, idx):
        return self.data[idx].astype(np.float32)


import tifffile as tiff
import os
import numpy as np

out_dir = "tiff_bandes"
os.makedirs(out_dir, exist_ok=True)

data = calibrated.spectral_albedo

for i, wl in enumerate(data.wavelength.values):

    band = data.isel(wavelength=i)  # (y, x)

    filename = f"{out_dir}/band_{int(wl)}nm.tif"

    tiff.imwrite(
        filename,
        band.values.astype(np.float32)
    )

import os
import matplotlib.pyplot as plt

out_dir = "images_bandes"
os.makedirs(out_dir, exist_ok=True)

cube = calibrated.spectral_albedo

n_bands = cube.shape[1]  # dimension wavelength

for i in range(n_bands):

    band = cube[:, i, :]   # (y, x)

    plt.figure()
    plt.imshow(band, cmap="gray")
    plt.title(f"Bande {i}")
    plt.axis("off")

    filename = f"{out_dir}/band_{i:03d}.png"
    plt.savefig(filename, dpi=300, bbox_inches="tight")
    plt.close()

import numpy as np
from sklearn.decomposition import PCA


# Cube hyperspectral
cube = calibrated.spectral_albedo   # (y, wl, x)

y, n_wl, x = cube.shape


# Reshape -> (pixels, bands)
X = cube.transpose(0, 2, 1).reshape(-1, n_wl)


# Nettoyage (√©viter NaN / inf)
X = np.nan_to_num(X)


# Normalisation (important pour PCA)
X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)


# PCA
pca = PCA(n_components=10)   # on garde 10 composantes
pca.fit(X)


# Importance des bandes
# = somme des valeurs absolues des poids
importance = np.sum(np.abs(pca.components_), axis=0)


# Classement
best_indices = np.argsort(importance)[::-1]


print("Bandes les plus importantes :")
print(best_indices[:10])

N = 8

selected = best_indices[:N]

print("Bandes s√©lectionn√©es :", selected)

import os
import matplotlib.pyplot as plt
from itertools import combinations


out_dir = "rgb_pca"
os.makedirs(out_dir, exist_ok=True)


def normalize(img):

    img = img.copy()

    for i in range(3):
        b = img[..., i]
        img[..., i] = (b - b.min()) / (b.max() - b.min() + 1e-8)

    return img


for r, g, b in combinations(selected, 3):

    R = cube[:, r, :]
    G = cube[:, g, :]
    B = cube[:, b, :]

    rgb = np.stack([R, G, B], axis=-1)

    rgb = normalize(rgb)

    name = f"rgb_{r}_{g}_{b}.png"

    plt.imsave(
        os.path.join(out_dir, name),
        rgb
    )

import matplotlib.pyplot as plt

plt.plot(importance)
plt.title("Importance PCA des longueurs d‚Äôonde")
plt.xlabel("Indice wavelength")
plt.ylabel("Importance")
plt.show()

try:
    wavelengths = np.array(spec.wavelengths)
except:
    wavelengths = np.array(spec.capture.wavelengths)

from sklearn.decomposition import PCA


y, n_wl, x = cube.shape

# reshape -> (pixels, bands)
X = cube.transpose(0, 2, 1).reshape(-1, n_wl)

# Nettoyage
X = np.nan_to_num(X)

# Normalisation
X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)


# PCA
pca = PCA(n_components=10)
pca.fit(X)


# Importance des bandes
importance = abs(pca.components_).sum(axis=0)

best_idx = importance.argsort()[::-1]

print("Top bandes :", best_idx[:10])

N = 6
selected = best_idx[:N]

import os
import matplotlib.pyplot as plt
from itertools import combinations


out_dir = "rgb_specarray_pca"
os.makedirs(out_dir, exist_ok=True)


def normalize(rgb):

    rgb = rgb.copy()

    for i in range(3):
        b = rgb[..., i]
        rgb[..., i] = (b - b.min()) / (b.max() - b.min() + 1e-8)

    return rgb


for r, g, b in combinations(selected, 3):

    R = cube[:, r, :]
    G = cube[:, g, :]
    B = cube[:, b, :]

    rgb = np.stack([R, G, B], axis=-1)

    rgb = normalize(rgb)

    # Nom avec longueur d'onde si possible
    try:
        name = f"rgb_{int(wavelengths[r])}_{int(wavelengths[g])}_{int(wavelengths[b])}.png"
    except:
        name = f"rgb_{r}_{g}_{b}.png"

    plt.imsave(os.path.join(out_dir, name), rgb)

pca_full = PCA().fit(X)

var = np.cumsum(pca_full.explained_variance_ratio_)

N = np.argmax(var > 0.95) + 1   # 95% variance


from specarray import SpecArray
from pathlib import Path
import numpy as np
import cv2
import matplotlib.pyplot as plt
import os

# -------------------------------
# 1Ô∏è‚É£ Lire les donn√©es avec specarray
# -------------------------------

data_path = Path("data/sample")  # chemin vers le dossier HSI
spec = SpecArray.from_folder(data_path)

# Si vous avez des references white/dark
# white = SpecArray.from_folder("data/white")
# dark  = SpecArray.from_folder("data/dark")
# spec = spec.compute_spectral_albedo(white=white, dark=dark)

cube = np.array(spec.spectral_albedo)  # (y, wavelength, x)
wavelengths = np.array(spec.wavelengths)  # vecteur des longueurs d'onde

y, n_wl, x = cube.shape
print(f"Cube shape: {cube.shape}")

# -------------------------------
# 2Ô∏è‚É£ Fonction pour appliquer CLAHE sur une bande
# -------------------------------

def apply_clahe(band, clip_limit=2.0, tile_size=(8,8)):
    """
    band : 2D array (y, x)
    """
    # Normaliser sur 0-255
    band_norm = cv2.normalize(band, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)
    enhanced = clahe.apply(band_norm)
    return enhanced

# Exemple sur une bande NIR (~index 30)
band_idx = 30
band = cube[:, band_idx, :]
band_clahe = apply_clahe(band)

plt.figure()
plt.imshow(band_clahe, cmap='gray')
plt.title(f"Bande {band_idx} CLAHE")
plt.axis('off')
plt.show()

# -------------------------------
# 3Ô∏è‚É£ Calculer des ratios spectraux
# -------------------------------

# Choisir 2 bandes pour le ratio (ex: 900nm et 1100nm)
# On peut choisir automatiquement les indices les plus proches
def find_nearest_idx(target_wl, wavelengths):
    return np.argmin(np.abs(wavelengths - target_wl))

wl1, wl2 = 900, 1100
idx1 = find_nearest_idx(wl1, wavelengths)
idx2 = find_nearest_idx(wl2, wavelengths)

band1 = cube[:, idx1, :]
band2 = cube[:, idx2, :]

# Ratio simple
ratio = band1 / (band2 + 1e-8)  # √©viter division par 0

# Normalisation pour affichage
ratio_norm = (ratio - ratio.min()) / (ratio.max() - ratio.min())

plt.figure()
plt.imshow(ratio_norm, cmap='gray')
plt.title(f"Ratio {wl1}nm/{wl2}nm")
plt.axis('off')
plt.show()

# -------------------------------
# 4Ô∏è‚É£ Sauvegarde
# -------------------------------

out_dir = "output_hsi"
os.makedirs(out_dir, exist_ok=True)

# Sauvegarde CLAHE
cv2.imwrite(os.path.join(out_dir, f"band_{band_idx}_clahe.png"), band_clahe)

# Sauvegarde ratio
plt.imsave(os.path.join(out_dir, f"ratio_{wl1}_{wl2}.png"), ratio_norm, cmap='gray')

print("Images sauvegard√©es dans", out_dir)

best_contrast = 0
best_pair = (0,0)
for i in range(n_wl):
    for j in range(n_wl):
        r = cube[:, i, :] / (cube[:, j, :] + 1e-8)
        contrast = r.std()  # ou r.max() - r.min()
        if contrast > best_contrast:
            best_contrast = contrast
            best_pair = (i,j)

import numpy as np
from tqdm import tqdm  # pour la barre de progression
import matplotlib.pyplot as plt

# cube : (y, n_wl, x)
# wavelengths : vecteur des longueurs d'onde

y, n_wl, x = cube.shape

def find_best_ratio(cube):
    best_score = -1
    best_pair = (0, 0)
    
    for i in tqdm(range(n_wl)):
        for j in range(n_wl):
            if i == j:
                continue  # √©viter la division par soi-m√™me
            band1 = cube[:, i, :]
            band2 = cube[:, j, :]
            
            ratio = band1 / (band2 + 1e-8)
            
            # normalisation
            ratio_norm = (ratio - ratio.min()) / (ratio.max() - ratio.min())
            
            # score bas√© sur l'√©cart type (contraste global)
            score = ratio_norm.std()
            
            if score > best_score:
                best_score = score
                best_pair = (i, j)
    
    return best_pair, best_score

# Calcul du meilleur ratio
best_pair, best_score = find_best_ratio(cube)
i_best, j_best = best_pair
print(f"Meilleur ratio : bandes {i_best} ({wavelengths[i_best]} nm) / {j_best} ({wavelengths[j_best]} nm), score={best_score:.4f}")

# Affichage du ratio optimal
band1 = cube[:, i_best, :]
band2 = cube[:, j_best, :]
ratio_opt = band1 / (band2 + 1e-8)
ratio_opt_norm = (ratio_opt - ratio_opt.min()) / (ratio_opt.max() - ratio_opt.min())

plt.figure()
plt.imshow(ratio_opt_norm, cmap='gray')
plt.title(f"Meilleur ratio {wavelengths[i_best]} nm / {wavelengths[j_best]} nm")
plt.axis('off')
plt.show()


from specarray import SpecArray
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import json
import cv2

data_path = Path("data/sample")
spec = SpecArray.from_folder(data_path)

cube = np.array(spec.spectral_albedo)
wavelengths = np.array(spec.wavelengths)

y, n_wl, x = cube.shape
print(f"Cube shape: {cube.shape}")

labelme_json = "annotations.json"

with open(labelme_json) as f:
    annotation = json.load(f)

target_class = "nom_de_ta_classe"
exclude_class = "trou"

mask = np.zeros((y, x), dtype=np.uint8)
mask_exclude = np.zeros((y, x), dtype=np.uint8)

for shape in annotation["shapes"]:
    polygon = np.array(shape["points"], dtype=np.int32)
    if shape["label"] == target_class:
        cv2.fillPoly(mask, [polygon], 1)
    if shape["label"] == exclude_class:
        cv2.fillPoly(mask_exclude, [polygon], 1)

mask[mask_exclude == 1] = 0

plt.figure()
plt.imshow(mask, cmap="gray")
plt.title(f"Masque classe: {target_class} sans trou")
plt.axis("off")
plt.show()

spectra = []

for i in range(y):
    for j in range(x):
        if mask[i, j] == 1:
            s = cube[i, :, j]
            if np.var(s) > 1e-8:
                spectra.append(s)

spectra = np.array(spectra)
print(f"Nombre de spectres extraits: {spectra.shape[0]}")

def find_nearest_idx(target_wl, wavelengths):
    return np.argmin(np.abs(wavelengths - target_wl))

idx_996 = find_nearest_idx(996, wavelengths)
idx_1197 = find_nearest_idx(1197, wavelengths)

print("Bande r√©elle proche de 996 nm:", wavelengths[idx_996])
print("Bande r√©elle proche de 1197 nm:", wavelengths[idx_1197])

plt.figure(figsize=(10,6))

for s in spectra:
    plt.plot(wavelengths, s, alpha=0.2)

mean_spectrum = spectra.mean(axis=0)
plt.plot(wavelengths, mean_spectrum, color="black", linewidth=3, label="Mean spectrum")

plt.axvline(wavelengths[idx_996], color="red", linestyle="--", label="996 nm")
plt.axvline(wavelengths[idx_1197], color="blue", linestyle="--", label="1197 nm")

plt.xlabel("Wavelength (nm)")
plt.ylabel("Reflectance")
plt.title(f"Spectres - Classe {target_class}")
plt.legend()
plt.show()

ratio_spectra = spectra[:, idx_996] / (spectra[:, idx_1197] + 1e-8)

plt.figure(figsize=(6,4))
plt.hist(ratio_spectra, bins=30)
plt.title(f"Distribution Ratio 996/1197 - {target_class}")
plt.xlabel("Ratio")
plt.ylabel("Frequency")
plt.show()

print("Ratio moyen:", ratio_spectra.mean())



import tkinter as tk
from PIL import Image, ImageTk
import numpy as np
import matplotlib.pyplot as plt
from specarray import SpecArray
from pathlib import Path

# -------------------------
# Charger cube hyperspectral
# -------------------------

data_path = Path("data/sample")
spec = SpecArray.from_folder(data_path)

cube = np.array(spec.spectral_albedo)
wavelengths = np.array(spec.wavelengths)

y, n_wl, x = cube.shape

def find_nearest_idx(target_wl, wavelengths):
    return np.argmin(np.abs(wavelengths - target_wl))

idx1 = find_nearest_idx(996, wavelengths)
idx2 = find_nearest_idx(1197, wavelengths)

band1 = cube[:, idx1, :]
band2 = cube[:, idx2, :]

ratio = band1 / (band2 + 1e-8)
ratio_norm = (ratio - ratio.min()) / (ratio.max() - ratio.min())
ratio_img = (ratio_norm * 255).astype(np.uint8)

# -------------------------
# Interface Tkinter
# -------------------------

class RatioViewer:

    def __init__(self, master, image_array):
        self.master = master
        self.master.title("Ratio Viewer")

        self.scale = 1.0
        self.original = image_array
        self.image = Image.fromarray(self.original)
        self.tk_image = ImageTk.PhotoImage(self.image)

        self.canvas = tk.Canvas(master, width=self.image.width, height=self.image.height)
        self.canvas.pack(fill=tk.BOTH, expand=True)

        self.image_on_canvas = self.canvas.create_image(0, 0, anchor=tk.NW, image=self.tk_image)

        self.canvas.bind("<Button-1>", self.on_click)
        self.canvas.bind("<MouseWheel>", self.zoom)

    def zoom(self, event):
        if event.delta > 0:
            self.scale *= 1.1
        else:
            self.scale /= 1.1

        new_size = (int(self.original.shape[1] * self.scale),
                    int(self.original.shape[0] * self.scale))

        resized = cv2.resize(self.original, new_size, interpolation=cv2.INTER_NEAREST)
        self.image = Image.fromarray(resized)
        self.tk_image = ImageTk.PhotoImage(self.image)

        self.canvas.config(width=new_size[0], height=new_size[1])
        self.canvas.itemconfig(self.image_on_canvas, image=self.tk_image)

    def on_click(self, event):
        x_click = int(event.x / self.scale)
        y_click = int(event.y / self.scale)

        if 0 <= x_click < x and 0 <= y_click < y:
            spectrum = cube[y_click, :, x_click]

            plt.figure()
            plt.plot(wavelengths, spectrum)
            plt.axvline(wavelengths[idx1], color='r')
            plt.axvline(wavelengths[idx2], color='b')
            plt.title(f"Spectrum at ({x_click},{y_click})")
            plt.xlabel("Wavelength (nm)")
            plt.ylabel("Reflectance")
            plt.show()

import cv2

root = tk.Tk()
viewer = RatioViewer(root, ratio_img)
root.mainloop()


import tkinter as tk
from tkinter import ttk
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import matplotlib.pyplot as plt
import numpy as np
from specarray import SpecArray
from pathlib import Path

data_path = Path("data/sample")
spec = SpecArray.from_folder(data_path)
cube = np.array(spec.spectral_albedo)  # (y, wavelength, x)
wavelengths = np.array(spec.wavelengths)
y, n_wl, x = cube.shape

class RatioViewer:
    def __init__(self, root, cube, wavelengths):
        self.root = root
        self.cube = cube
        self.wavelengths = wavelengths
        self.idx_red = 0
        self.idx_blue = 0
        self.fig, self.ax = plt.subplots(figsize=(6,6))
        self.canvas = FigureCanvasTkAgg(self.fig, master=root)
        self.canvas.get_tk_widget().grid(row=1, column=0, columnspan=4)
        self.ratio_image = None

        tk.Label(root, text="Red band (nm)").grid(row=0, column=0)
        tk.Label(root, text="Blue band (nm)").grid(row=0, column=2)

        self.red_entry = tk.Entry(root)
        self.red_entry.grid(row=0, column=1)
        self.red_entry.insert(0, str(int(wavelengths[0])))

        self.blue_entry = tk.Entry(root)
        self.blue_entry.grid(row=0, column=3)
        self.blue_entry.insert(0, str(int(wavelengths[-1])))

        tk.Button(root, text="Update Ratio", command=self.update_ratio).grid(row=0, column=4)

        self.canvas.mpl_connect("button_press_event", self.on_click)
        self.update_ratio()

    def find_nearest_idx(self, target_wl):
        return np.argmin(np.abs(self.wavelengths - target_wl))

    def update_ratio(self):
        red_nm = float(self.red_entry.get())
        blue_nm = float(self.blue_entry.get())
        self.idx_red = self.find_nearest_idx(red_nm)
        self.idx_blue = self.find_nearest_idx(blue_nm)

        band_red = self.cube[:, self.idx_red, :]
        band_blue = self.cube[:, self.idx_blue, :]
        ratio = band_red / (band_blue + 1e-8)
        ratio = (ratio - ratio.min()) / (ratio.max() - ratio.min())

        self.ax.clear()
        self.ratio_image = ratio
        self.ax.imshow(ratio, cmap="gray")
        self.ax.set_title(f"Ratio {red_nm:.0f}nm / {blue_nm:.0f}nm")
        self.canvas.draw()

    def on_click(self, event):
        if event.inaxes != self.ax:
            return
        j, i = int(event.xdata + 0.5), int(event.ydata + 0.5)
        if 0 <= i < self.cube.shape[0] and 0 <= j < self.cube.shape[2]:
            spectrum = self.cube[i, :, j]
            plt.figure()
            plt.plot(self.wavelengths, spectrum)
            plt.axvline(self.wavelengths[self.idx_red], color="red", linestyle="--")
            plt.axvline(self.wavelengths[self.idx_blue], color="blue", linestyle="--")
            plt.title(f"Pixel ({i},{j}) spectrum")
            plt.xlabel("Wavelength (nm)")
            plt.ylabel("Reflectance")
            plt.show()

root = tk.Tk()
root.title("Hyperspectral Ratio Viewer")
viewer = RatioViewer(root, cube, wavelengths)
root.mainloop()


import tkinter as tk
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import matplotlib.pyplot as plt
from specarray import SpecArray
import numpy as np
from pathlib import Path

class RatioViewer:
    def __init__(self, master, spec_folder):
        self.master = master
        self.spec = SpecArray.from_folder(Path(spec_folder))
        self.cube = np.array(self.spec.spectral_albedo)  # (y, wavelength, x)
        self.wavelengths = np.array(self.spec.wavelengths)
        self.y, self.n_wl, self.x = self.cube.shape
        self.idx_red = 0
        self.idx_blue = 0

        self.fig, (self.ax_ratio, self.ax_spectrum) = plt.subplots(1, 2, figsize=(10,5))
        self.canvas = FigureCanvasTkAgg(self.fig, master=master)
        self.canvas.get_tk_widget().pack()

        self.red_label = tk.Label(master, text="Red nm:")
        self.red_label.pack(side="left")
        self.red_entry = tk.Entry(master)
        self.red_entry.pack(side="left")
        self.red_entry.insert(0, str(self.wavelengths[0]))

        self.blue_label = tk.Label(master, text="Blue nm:")
        self.blue_label.pack(side="left")
        self.blue_entry = tk.Entry(master)
        self.blue_entry.pack(side="left")
        self.blue_entry.insert(0, str(self.wavelengths[10]))

        self.update_button = tk.Button(master, text="Update Ratio", command=self.update_ratio)
        self.update_button.pack(side="left")

        self.canvas.mpl_connect("button_press_event", self.on_click)
        self.update_ratio()

    def find_nearest_idx(self, target_nm):
        return np.argmin(np.abs(self.wavelengths - target_nm))

    def update_ratio(self):
        red_nm = float(self.red_entry.get())
        blue_nm = float(self.blue_entry.get())
        self.idx_red = self.find_nearest_idx(red_nm)
        self.idx_blue = self.find_nearest_idx(blue_nm)

        band_red = self.cube[:, self.idx_red, :]
        band_blue = self.cube[:, self.idx_blue, :]
        ratio = band_red / (band_blue + 1e-8)
        ratio = (ratio - ratio.min()) / (ratio.max() - ratio.min())
        self.ratio_image = ratio

        self.ax_ratio.clear()
        self.ax_ratio.imshow(ratio, cmap="gray", vmin=0, vmax=1)
        self.ax_ratio.set_title(f"Ratio {red_nm:.0f}nm / {blue_nm:.0f}nm")

        self.ax_spectrum.clear()
        self.ax_spectrum.set_ylim(0, 1)  # fixe l'axe vertical pour spectre
        self.ax_spectrum.set_title("Spectre du pixel cliqu√©")
        self.canvas.draw()

    def on_click(self, event):
        if event.inaxes != self.ax_ratio:
            return
        x_pix = int(event.xdata)
        y_pix = int(event.ydata)
        s = self.cube[y_pix, :, x_pix]

        self.ax_spectrum.clear()
        self.ax_spectrum.plot(self.wavelengths, s, color="blue")
        self.ax_spectrum.set_ylim(0, 1)  # axe vertical fixe
        self.ax_spectrum.set_xlabel("Wavelength (nm)")
        self.ax_spectrum.set_ylabel("Reflectance")
        self.ax_spectrum.set_title(f"Pixel ({x_pix}, {y_pix})")
        self.canvas.draw()


if __name__ == "__main__":
    root = tk.Tk()
    viewer = RatioViewer(root, "data/sample")
    root.mainloop()


import numpy as np
import matplotlib.pyplot as plt
from specarray import SpecArray
from pathlib import Path

data_path = Path("data/sample")
spec = SpecArray.from_folder(data_path)

cube = np.array(spec.spectral_albedo)  # (y, wavelength, x)
wavelengths = np.array(spec.wavelengths)

y, n_wl, x = cube.shape
pixels = cube.reshape(y*x, n_wl)

corr_matrix = np.corrcoef(pixels.T)

plt.figure(figsize=(12,10))
im = plt.imshow(corr_matrix, cmap='bwr', vmin=-1, vmax=1)

plt.colorbar(im, label='Correlation coefficient')
plt.title("Carte de corr√©lation entre bandes")
plt.xlabel("Wavelength (nm)")
plt.ylabel("Wavelength (nm)")

# Choisir par exemple 10 ticks espac√©s uniform√©ment
num_ticks = 10
tick_indices = np.linspace(0, n_wl-1, num_ticks, dtype=int)
tick_labels = np.round(wavelengths[tick_indices]).astype(int)

plt.xticks(ticks=tick_indices, labels=tick_labels, rotation=45)
plt.yticks(ticks=tick_indices, labels=tick_labels)

plt.show()


from specarray import SpecArray
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
import json
import cv2
from itertools import combinations

data_path = Path("data/sample")
spec = SpecArray.from_folder(data_path)
cube = np.array(spec.spectral_albedo)
wavelengths = np.array(spec.wavelengths)
y, n_wl, x = cube.shape

labelme_json = "annotations.json"
with open(labelme_json) as f:
    annotation = json.load(f)

classes_to_ignore = ["trou", "plot"]
class_masks = {}
mask_total = np.zeros((y, x), dtype=np.uint8)

for shape in annotation["shapes"]:
    label = shape["label"]
    polygon = np.array(shape["points"], dtype=np.int32)
    if label in classes_to_ignore:
        cv2.fillPoly(mask_total, [polygon], 1)
        continue
    if label not in class_masks:
        class_masks[label] = np.zeros((y, x), dtype=np.uint8)
    cv2.fillPoly(class_masks[label], [polygon], 1)
    mask_total[class_masks[label] == 1] = 1

mask_background = mask_total == 0

def compute_contrast_ratio(cube, mask_obj, mask_bg):
    contrast_scores = np.zeros((n_wl, n_wl))
    for i,j in combinations(range(n_wl),2):
        band_i = cube[:, i, :]
        band_j = cube[:, j, :]
        ratio = band_i / (band_j + 1e-8)
        obj_pixels = ratio[mask_obj]
        bg_pixels  = ratio[mask_bg]
        score = np.abs(obj_pixels.mean() - bg_pixels.mean())
        contrast_scores[i,j] = score
    return contrast_scores

top_n = 5
for label, mask_obj in class_masks.items():
    scores = compute_contrast_ratio(cube, mask_obj, mask_background)
    idx_flat = np.argsort(scores.flatten())[::-1]
    pairs = [np.unravel_index(idx, scores.shape) for idx in idx_flat[:top_n]]
    print(f"Classe {label} - top {top_n} paires bandes (wl1, wl2) avec meilleur contraste:")
    for (i,j) in pairs:
        print(f"{wavelengths[i]:.1f} nm / {wavelengths[j]:.1f} nm -> score {scores[i,j]:.4f}")

        ratio_img = cube[:, i, :] / (cube[:, j, :] + 1e-8)
        ratio_norm = (ratio_img - ratio_img.min()) / (ratio_img.max() - ratio_img.min())
        plt.figure(figsize=(5,5))
        plt.imshow(ratio_norm, cmap="gray")
        plt.title(f"{label} - Ratio {wavelengths[i]:.1f}/{wavelengths[j]:.1f}")
        plt.axis("off")
        plt.show()
